{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b9e729",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d4f7c",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "This chapter will introduce about the **Embedding**.\n",
    "\n",
    "\n",
    "We have the tokenizer now. It will recieve a text, and convert it to a list of index. As we begin the tokenizer, we have a intro to build a dictionary. Yeah, what tokenizer do is to look up the dictionary and return the list of index. However this index is discreted. Take a example the number $1$ and the number $2$. Between them, we have many numbers like $1.1, 1.1123, 1.3, 1.5, 1.679, ...$. So we say the integer index is discreted. And this is not good for representing semantics. Like the word `cat`'s index is 100, and the index of word `dog` is 200, and the index of word `car` is 300. We know `cat` and `dog` is more similar, they are animals. But they `dog` has the same distance to `cat` and `car`. We need some method that is continuous to demenstrate those semantics, so there comes **embedding**.\n",
    "\n",
    "\n",
    "In LLM, the embedding recieves the output of tokenizer's encode the index of the text, and embedding will generate a matrix or 2 dimension tensor. How it could be? In truth we can see embedding as a table, and in the table all are float-point number. They are continuous. And with a index you will find one row vector to it. It likes a index table. \n",
    "\n",
    "Now we will feel that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556d4456",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1 token embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673833b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79d22a9",
   "metadata": {},
   "source": [
    "Let's import tokenizer and the prepared corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bc25388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import ast\n",
    "from module.code.tokenizer import Tokenizer\n",
    "\n",
    "with open(\"../corpus/pretext.txt\", 'r', encoding=\"utf-8\") as file:\n",
    "    str_vocab = file.read()\n",
    "\n",
    "list_vocab = ast.literal_eval(str_vocab)\n",
    "\n",
    "length = len(list_vocab)\n",
    "\n",
    "tokenizer = Tokenizer(list_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d2cb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([134, 4])\n",
      "0: tensor([0.9217, 0.2847, 0.5199, 0.6047])\n",
      "1: tensor([0.0738, 0.3831, 0.8120, 0.5090])\n",
      "2: tensor([0.4314, 0.4678, 0.0935, 0.5897])\n",
      "3: tensor([0.0644, 0.6565, 0.9434, 0.9326])\n",
      "4: tensor([0.5674, 0.8739, 0.7428, 0.2756])\n",
      "5: tensor([0.8946, 0.3077, 0.7842, 0.6342])\n",
      "6: tensor([0.3387, 0.6168, 0.4277, 0.4966])\n",
      "7: tensor([0.0829, 0.2089, 0.8593, 0.9363])\n",
      "8: tensor([0.3632, 0.3655, 0.4200, 0.4381])\n",
      "9: tensor([0.5909, 0.2297, 0.5391, 0.3824])\n",
      "10: tensor([0.4746, 0.7611, 0.1532, 0.8853])\n",
      "11: tensor([0.2583, 0.0882, 0.6690, 0.6178])\n",
      "12: tensor([0.3439, 0.8106, 0.7882, 0.7749])\n",
      "13: tensor([0.3926, 0.1875, 0.8925, 0.5845])\n",
      "14: tensor([0.5174, 0.0086, 0.7794, 0.7607])\n",
      "15: tensor([0.9466, 0.6627, 0.4734, 0.8191])\n",
      "16: tensor([0.4282, 0.4492, 0.6313, 0.9083])\n",
      "17: tensor([0.9886, 0.2532, 0.3203, 0.8473])\n",
      "18: tensor([0.4188, 0.9219, 0.8381, 0.3018])\n",
      "19: tensor([0.1925, 0.4732, 0.1322, 0.7411])\n",
      "20: tensor([0.2841, 0.9052, 0.3563, 0.1473])\n",
      "21: tensor([0.7996, 0.0381, 0.4922, 0.6123])\n",
      "22: tensor([0.3580, 0.4069, 0.4543, 0.7241])\n",
      "23: tensor([0.5726, 0.1783, 0.0928, 0.3897])\n",
      "24: tensor([0.2678, 0.9335, 0.9153, 0.4088])\n",
      "25: tensor([0.9383, 0.5478, 0.7976, 0.2298])\n",
      "26: tensor([0.2263, 0.4640, 0.9725, 0.8929])\n",
      "27: tensor([0.5793, 0.5219, 0.6048, 0.9117])\n",
      "28: tensor([0.5344, 0.0567, 0.5162, 0.5661])\n",
      "29: tensor([0.9408, 0.7899, 0.5962, 0.2092])\n",
      "30: tensor([0.4900, 0.5150, 0.2236, 0.7822])\n",
      "31: tensor([0.7147, 0.4212, 0.1783, 0.9261])\n",
      "32: tensor([0.6620, 0.5358, 0.1568, 0.4109])\n",
      "33: tensor([0.3171, 0.9107, 0.9710, 0.4259])\n",
      "34: tensor([0.3947, 0.1967, 0.9917, 0.7138])\n",
      "35: tensor([0.6386, 0.5200, 0.8779, 0.3204])\n",
      "36: tensor([0.9469, 0.9803, 0.6119, 0.2435])\n",
      "37: tensor([0.9890, 0.4896, 0.9200, 0.7428])\n",
      "38: tensor([0.3485, 0.5073, 0.5304, 0.4934])\n",
      "39: tensor([0.5451, 0.3459, 0.0765, 0.1857])\n",
      "40: tensor([0.9271, 0.4574, 0.3070, 0.5212])\n",
      "41: tensor([0.4172, 0.9130, 0.7882, 0.8408])\n",
      "42: tensor([0.1190, 0.9984, 0.8784, 0.4252])\n",
      "43: tensor([0.4769, 0.9719, 0.4015, 0.1720])\n",
      "44: tensor([0.8946, 0.9460, 0.7957, 0.3741])\n",
      "45: tensor([0.7492, 0.2026, 0.9128, 0.4885])\n",
      "46: tensor([0.7231, 0.0814, 0.5142, 0.2555])\n",
      "47: tensor([0.0250, 0.9376, 0.7016, 0.5200])\n",
      "48: tensor([0.1025, 0.9598, 0.4206, 0.8983])\n",
      "49: tensor([0.0957, 0.9367, 0.2111, 0.1334])\n",
      "50: tensor([0.5124, 0.2808, 0.5548, 0.0019])\n",
      "51: tensor([0.8545, 0.5321, 0.3324, 0.1139])\n",
      "52: tensor([0.9228, 0.4124, 0.2925, 0.0246])\n",
      "53: tensor([0.6776, 0.7439, 0.4741, 0.1671])\n",
      "54: tensor([0.3962, 0.5246, 0.5895, 0.5898])\n",
      "55: tensor([0.0615, 0.4750, 0.6585, 0.4083])\n",
      "56: tensor([0.4825, 0.5853, 0.8785, 0.2248])\n",
      "57: tensor([0.5755, 0.0298, 0.9040, 0.6738])\n",
      "58: tensor([0.2931, 0.6655, 0.5696, 0.2135])\n",
      "59: tensor([0.0797, 0.1771, 0.1887, 0.8282])\n",
      "60: tensor([0.1512, 0.5037, 0.0868, 0.7647])\n",
      "61: tensor([0.4182, 0.8529, 0.6958, 0.6782])\n",
      "62: tensor([0.3157, 0.5826, 0.5361, 0.7773])\n",
      "63: tensor([0.8119, 0.4424, 0.0170, 0.1425])\n",
      "64: tensor([0.9829, 0.8504, 0.1632, 0.0038])\n",
      "65: tensor([0.1869, 0.4816, 0.0882, 0.4399])\n",
      "66: tensor([0.8687, 0.1665, 0.1197, 0.8138])\n",
      "67: tensor([0.1505, 0.9162, 0.2635, 0.4815])\n",
      "68: tensor([0.3084, 0.6455, 0.6842, 0.7708])\n",
      "69: tensor([0.1239, 0.5843, 0.2422, 0.7993])\n",
      "70: tensor([0.4893, 0.9376, 0.1524, 0.6225])\n",
      "71: tensor([0.1622, 0.9456, 0.0280, 0.6030])\n",
      "72: tensor([0.9790, 0.6417, 0.6952, 0.6772])\n",
      "73: tensor([0.3844, 0.6436, 0.1176, 0.1832])\n",
      "74: tensor([0.8748, 0.6578, 0.0708, 0.5040])\n",
      "75: tensor([0.3629, 0.6937, 0.9596, 0.7675])\n",
      "76: tensor([0.3679, 0.4972, 0.6351, 0.2955])\n",
      "77: tensor([0.1425, 0.1910, 0.4719, 0.6541])\n",
      "78: tensor([0.2968, 0.6456, 0.6363, 0.5222])\n",
      "79: tensor([0.7843, 0.3542, 0.0552, 0.0845])\n",
      "80: tensor([0.3771, 0.1431, 0.1965, 0.9097])\n",
      "81: tensor([0.1601, 0.1290, 0.3281, 0.1677])\n",
      "82: tensor([0.4926, 0.6585, 0.9419, 0.3180])\n",
      "83: tensor([0.9650, 0.4868, 0.3170, 0.2388])\n",
      "84: tensor([0.1746, 0.3568, 0.7396, 0.9776])\n",
      "85: tensor([0.3867, 0.5422, 0.5684, 0.4666])\n",
      "86: tensor([0.5910, 0.7852, 0.4745, 0.6468])\n",
      "87: tensor([0.0402, 0.6321, 0.0643, 0.8193])\n",
      "88: tensor([0.3328, 0.8186, 0.0482, 0.9240])\n",
      "89: tensor([0.4964, 0.6505, 0.3990, 0.9538])\n",
      "90: tensor([0.7975, 0.8929, 0.0787, 0.1034])\n",
      "91: tensor([0.4964, 0.6707, 0.8689, 0.7710])\n",
      "92: tensor([0.6922, 0.4004, 0.1162, 0.6191])\n",
      "93: tensor([0.4334, 0.3060, 0.3528, 0.0703])\n",
      "94: tensor([0.2802, 0.7049, 0.2338, 0.0142])\n",
      "95: tensor([0.3148, 0.4394, 0.2112, 0.3725])\n",
      "96: tensor([0.2534, 0.8435, 0.0812, 0.4564])\n",
      "97: tensor([0.5464, 0.5092, 0.7910, 0.0221])\n",
      "98: tensor([0.4473, 0.8503, 0.9295, 0.9472])\n",
      "99: tensor([0.6866, 0.2427, 0.0931, 0.9953])\n",
      "100: tensor([0.0770, 0.6325, 0.8906, 0.6058])\n",
      "101: tensor([0.3712, 0.4129, 0.2614, 0.9901])\n",
      "102: tensor([0.5773, 0.4552, 0.0060, 0.6667])\n",
      "103: tensor([0.0375, 0.7357, 0.1510, 0.4463])\n",
      "104: tensor([0.8932, 0.2723, 0.0629, 0.0393])\n",
      "105: tensor([0.6804, 0.1442, 0.4879, 0.9988])\n",
      "106: tensor([0.2753, 0.7374, 0.9311, 0.0742])\n",
      "107: tensor([0.4221, 0.5444, 0.2806, 0.1584])\n",
      "108: tensor([0.7448, 0.6926, 0.1916, 0.4865])\n",
      "109: tensor([0.3329, 0.5100, 0.7082, 0.2567])\n",
      "110: tensor([0.6655, 0.9384, 0.1265, 0.7799])\n",
      "111: tensor([0.5316, 0.6459, 0.0168, 0.9640])\n",
      "112: tensor([0.2602, 0.9042, 0.9936, 0.1673])\n",
      "113: tensor([0.3563, 0.6604, 0.5751, 0.2682])\n",
      "114: tensor([0.7776, 0.9069, 0.7028, 0.5715])\n",
      "115: tensor([0.6861, 0.1580, 0.3751, 0.0115])\n",
      "116: tensor([0.4100, 0.5390, 0.5668, 0.2846])\n",
      "117: tensor([0.6634, 0.7777, 0.6068, 0.5630])\n",
      "118: tensor([0.2207, 0.7741, 0.3101, 0.3289])\n",
      "119: tensor([0.0727, 0.0826, 0.8129, 0.1091])\n",
      "120: tensor([0.8653, 0.1390, 0.9417, 0.9779])\n",
      "121: tensor([0.3150, 0.6937, 0.6958, 0.7478])\n",
      "122: tensor([0.5512, 0.7940, 0.3408, 0.1667])\n",
      "123: tensor([0.9832, 0.1347, 0.7283, 0.0658])\n",
      "124: tensor([0.1349, 0.6040, 0.1159, 0.4488])\n",
      "125: tensor([0.5318, 0.1478, 0.6485, 0.0900])\n",
      "126: tensor([0.1336, 0.1674, 0.2414, 0.7787])\n",
      "127: tensor([0.9863, 0.3026, 0.6688, 0.8363])\n",
      "128: tensor([0.5748, 0.7235, 0.3643, 0.0031])\n",
      "129: tensor([0.2200, 0.7934, 0.1132, 0.2370])\n",
      "130: tensor([0.0593, 0.5257, 0.9084, 0.9858])\n",
      "131: tensor([0.2634, 0.5449, 0.9440, 0.2584])\n",
      "132: tensor([0.4737, 0.5361, 0.4853, 0.9429])\n",
      "133: tensor([0.4805, 0.0930, 0.9353, 0.8752])\n"
     ]
    }
   ],
   "source": [
    "# Set rand seed, and the result will easily repeat\n",
    "torch.manual_seed(89)\n",
    "\n",
    "embed = torch.rand((length, 4))\n",
    "\n",
    "print(embed.shape)\n",
    "\n",
    "for index in range(length):\n",
    "    print(f\"{index}: {embed[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2550805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 35, 14, 58, 132, 127, 11]\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I hold you with?\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a782c4",
   "metadata": {},
   "source": [
    "print the first row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2b8468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9217, 0.2847, 0.5199, 0.6047])\n"
     ]
    }
   ],
   "source": [
    "print(embed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df13a04",
   "metadata": {},
   "source": [
    "then, print the ids list Corresponding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02ca393c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4282, 0.4492, 0.6313, 0.9083],\n",
      "        [0.6386, 0.5200, 0.8779, 0.3204],\n",
      "        [0.5174, 0.0086, 0.7794, 0.7607],\n",
      "        [0.2931, 0.6655, 0.5696, 0.2135],\n",
      "        [0.4737, 0.5361, 0.4853, 0.9429],\n",
      "        [0.9863, 0.3026, 0.6688, 0.8363],\n",
      "        [0.2583, 0.0882, 0.6690, 0.6178]])\n"
     ]
    }
   ],
   "source": [
    "print(embed[ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8ad764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: tensor([0.4282, 0.4492, 0.6313, 0.9083])\n",
      "35: tensor([0.6386, 0.5200, 0.8779, 0.3204])\n",
      "14: tensor([0.5174, 0.0086, 0.7794, 0.7607])\n",
      "58: tensor([0.2931, 0.6655, 0.5696, 0.2135])\n",
      "132: tensor([0.4737, 0.5361, 0.4853, 0.9429])\n",
      "127: tensor([0.9863, 0.3026, 0.6688, 0.8363])\n",
      "11: tensor([0.2583, 0.0882, 0.6690, 0.6178])\n"
     ]
    }
   ],
   "source": [
    "for id in ids:\n",
    "    print(f\"{id}: {embed[id]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcc5251",
   "metadata": {},
   "source": [
    "Actually, like what we say above, it's a index table. With the index and you can find it's vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65afbe",
   "metadata": {},
   "source": [
    "That's it! May you feel a little bit curious. Aha, then the embedding ends? It is and it is not. Actually in the LLM, the embedding will be trained. In practice, we will give it a sentence, and let it to predict the next word is. That's what the LLM do. Given some word, and predixt the next word, then append the new word to the original sentence, then go on predict the next word. So In the progress, we need to give the LLM with original sentence(input), and the next word. The next word is our target.\n",
    "\n",
    "So we need to prepare them the original sentence and the next word. Actually in the LLM, we will do like this. Given the sentence **\"What does LLM do to structure that\"**.  We use a `sliding window`. Assuming the window can contain 4 words. And the first input is: **\"What does LLM do\"**, and it's target is: **\"does LLM do to\"**. Then the second input is: **\"does LLM do to\"**, corresponding target is: **\"LLM do to structure\"**. That's it, the window will move one step to generate the target based on the input.\n",
    "\n",
    "And the description is drawed to a picture as below.\n",
    "\n",
    "<center><img src=\"https://github.com/gzqccnu/img/blob/main/LLM-input-target.png?raw=true\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce525c",
   "metadata": {},
   "source": [
    "We use `context_size` to replace how many words the sliding window contaian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0524ca",
   "metadata": {},
   "source": [
    "First of all, we import `re` for we must have the original text spliting to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "755af5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f91759f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:['What', ' ', 'does', ' '] ==> target: [' ', 'does', ' ', 'LLM']\n",
      "input:[' ', 'does', ' ', 'LLM'] ==> target: ['does', ' ', 'LLM', ' ']\n",
      "input:['does', ' ', 'LLM', ' '] ==> target: [' ', 'LLM', ' ', 'do']\n",
      "input:[' ', 'LLM', ' ', 'do'] ==> target: ['LLM', ' ', 'do', ' ']\n",
      "input:['LLM', ' ', 'do', ' '] ==> target: [' ', 'do', ' ', 'to']\n",
      "input:[' ', 'do', ' ', 'to'] ==> target: ['do', ' ', 'to', ' ']\n",
      "input:['do', ' ', 'to', ' '] ==> target: [' ', 'to', ' ', 'structure']\n",
      "input:[' ', 'to', ' ', 'structure'] ==> target: ['to', ' ', 'structure', ' ']\n",
      "input:['to', ' ', 'structure', ' '] ==> target: [' ', 'structure', ' ', 'that']\n",
      "input:[' ', 'structure', ' ', 'that'] ==> target: ['structure', ' ', 'that']\n",
      "input:['structure', ' ', 'that'] ==> target: [' ', 'that']\n",
      "input:[' ', 'that'] ==> target: ['that']\n",
      "input:['that'] ==> target: []\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "orig_stc = \"What does LLM do to structure that\"\n",
    "preprocess_stc = re.split('(\\s|[,.;:\\'\\\"()])', orig_stc)\n",
    "\n",
    "for i in range(len(preprocess_stc)):\n",
    "    print(f\"input:{preprocess_stc[i: i + context_size]} ==> target: {preprocess_stc[i + 1: i + 1 + context_size]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fbdef5",
   "metadata": {},
   "source": [
    "To encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c292170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:[16, 10, 10, 10] ==> target: [10, 10, 10, 116]\n",
      "input:[10, 10, 10, 116] ==> target: [10, 10, 116, 10]\n",
      "input:[10, 10, 116, 10] ==> target: [10, 116, 10, 110]\n",
      "input:[10, 116, 10, 110] ==> target: [116, 10, 110]\n",
      "input:[116, 10, 110] ==> target: [10, 110]\n",
      "input:[10, 110] ==> target: [110]\n",
      "input:[110] ==> target: []\n"
     ]
    }
   ],
   "source": [
    "# Attention: here we must use the original setence as input\n",
    "encoded_stc = tokenizer.encode(orig_stc)\n",
    "\n",
    "for i in range(len(encoded_stc)):\n",
    "    print(f\"input:{encoded_stc[i: i + context_size]} ==> target: {encoded_stc[i + 1: i + 1 + context_size]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055196e2",
   "metadata": {},
   "source": [
    "mock the progress of generating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a4ab510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] == > 16\n",
      "[16] == > 10\n",
      "[16, 10] == > 10\n",
      "[16, 10, 10] == > 10\n",
      "[16, 10, 10, 10] == > 116\n",
      "[16, 10, 10, 10, 116] == > 10\n",
      "[16, 10, 10, 10, 116, 10] == > 110\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(encoded_stc)):\n",
    "    print(f\"{encoded_stc[:i]} == > {encoded_stc[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ae0ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==> What\n",
      "What ==> <|unk|>\n",
      "What <|unk|> ==> <|unk|>\n",
      "What <|unk|> <|unk|> ==> <|unk|>\n",
      "What <|unk|> <|unk|> <|unk|> ==> to\n",
      "What <|unk|> <|unk|> <|unk|> to ==> <|unk|>\n",
      "What <|unk|> <|unk|> <|unk|> to <|unk|> ==> that\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(encoded_stc)):\n",
    "    print(f\"{tokenizer.decode(encoded_stc[:i])} ==> {tokenizer.decode([encoded_stc[i]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99e6485",
   "metadata": {},
   "source": [
    "Note that: here our corpus is a poetry, so it will look like very curious.\n",
    "\n",
    "Let's has the first sentence of the poetry to have a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9d1552d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ==> What\n",
      "What ==> can\n",
      "What can ==> I\n",
      "What can I ==> hold\n",
      "What can I hold ==> you\n",
      "What can I hold you ==> with\n",
      "What can I hold you with ==> ?\n"
     ]
    }
   ],
   "source": [
    "text = \"What can I hold you with?\"\n",
    "\n",
    "encoded_text = tokenizer.encode(text)\n",
    "\n",
    "for i in range(len(encoded_text)):\n",
    "    print(f\"{tokenizer.decode(encoded_text[:i])} ==> {tokenizer.decode([encoded_text[i]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dce826",
   "metadata": {},
   "source": [
    "Here, we can also use `PyTorch`'s embedding. And it requires two parameters. First is the `vocabulary size` then is the `output_dim`. The `output_dim` dedicates how long the embeded vector is after embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82aaf588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.5620e-01, -9.0666e-02, -2.2929e-01, -2.8675e-01,  1.2585e+00,\n",
      "         -4.6703e-01,  4.9767e-01,  9.7761e-01, -6.9830e-01,  1.1584e+00,\n",
      "         -6.2008e-01, -1.3283e+00, -1.6209e+00, -2.4759e+00, -6.3094e-02,\n",
      "          6.2485e-02,  1.6153e+00,  5.6440e-01, -6.1894e-01, -1.6521e+00,\n",
      "         -2.5804e-01, -4.0598e-01, -1.7543e+00, -6.0936e-01,  1.1187e+00,\n",
      "         -6.3776e-01, -2.8997e-01,  2.6506e-01,  7.7799e-01, -1.1255e+00,\n",
      "         -1.0479e-01, -9.3177e-01,  1.1920e+00,  1.4861e+00, -1.0375e+00,\n",
      "          1.5590e+00,  4.8431e-01,  1.7005e+00, -1.9703e+00, -1.4964e-01,\n",
      "          9.1853e-01, -8.2487e-01,  2.2310e+00, -5.8567e-01, -1.1222e+00,\n",
      "         -8.3964e-01,  6.5284e-01, -4.3925e-01, -7.0254e-01, -1.2963e+00,\n",
      "         -7.7522e-01,  1.0023e+00, -5.2820e-01,  1.1389e+00,  2.4697e+00,\n",
      "          2.0219e+00,  1.5224e-01,  4.8015e-01, -1.7240e-01,  1.5409e+00,\n",
      "         -1.2310e-01, -1.3589e+00,  4.5504e-01, -1.6065e+00,  1.7667e+00,\n",
      "         -1.2207e+00, -1.2935e-01,  8.0965e-01, -7.1760e-01,  1.8482e+00,\n",
      "          4.8983e-01,  2.1291e+00,  1.7642e+00, -1.4663e+00,  1.3488e-01,\n",
      "          5.7239e-01,  1.1494e+00,  5.0576e-01, -5.1584e-01, -9.8896e-01,\n",
      "          1.5916e+00, -2.4757e-01,  2.5970e-01,  5.3005e-01,  9.4621e-01,\n",
      "          4.0085e-01, -6.7802e-01,  8.2098e-01, -3.9634e-01,  4.7302e-01,\n",
      "         -8.3599e-02, -9.5251e-02,  7.1799e-01,  7.9591e-03, -1.5680e+00,\n",
      "          2.8247e-03, -4.1261e-01,  6.1355e-01,  1.2462e+00, -1.1644e+00,\n",
      "         -1.7018e+00,  3.1270e-01, -9.8993e-01,  8.1269e-01,  1.2280e+00,\n",
      "          1.1039e+00,  3.5304e-01,  1.3416e+00,  5.2498e-02,  2.3994e-01,\n",
      "         -6.1954e-02,  9.8283e-02, -1.5402e-01, -5.5118e-01,  3.0265e-01,\n",
      "         -5.5733e-01, -4.6456e-01,  2.6269e-01,  1.2955e+00,  9.7127e-01,\n",
      "         -2.0122e+00, -1.4739e+00, -2.1859e-01,  9.5501e-01,  9.8628e-01,\n",
      "          4.7932e-01, -8.0001e-02, -1.1753e-01],\n",
      "        [ 8.8929e-01,  1.2706e+00, -5.0707e-01,  1.6297e+00,  2.2532e+00,\n",
      "         -1.0448e-01,  5.1417e-01,  3.3458e-01,  3.5485e-01, -1.4447e+00,\n",
      "          3.2144e-01, -8.5007e-01, -1.4888e+00,  1.2608e+00,  2.6989e+00,\n",
      "          2.5848e-01,  1.3746e-01, -5.3252e-01,  1.7816e+00,  1.2122e+00,\n",
      "          3.5902e-02,  9.6671e-01,  1.2920e+00, -9.1846e-01,  8.2008e-01,\n",
      "         -1.1869e+00, -6.2326e-01, -3.0747e-01,  9.3115e-01, -5.2618e-01,\n",
      "          1.0618e-01,  2.6503e-01, -2.0154e-01, -2.0020e-01,  1.6004e+00,\n",
      "         -6.8864e-01,  2.2421e-01, -2.1880e+00,  3.0721e-01,  2.3164e-01,\n",
      "          5.4476e-01,  3.3234e-01, -1.1262e+00,  1.7455e+00,  7.9042e-02,\n",
      "         -9.1665e-01,  1.9266e+00, -1.0844e+00, -1.9829e+00, -3.2300e-01,\n",
      "         -1.5056e+00,  5.5248e-01, -1.2837e+00,  4.3387e-01,  8.5515e-01,\n",
      "          1.9461e-02, -1.0492e-01, -5.3472e-01,  2.2775e-01,  7.3188e-01,\n",
      "          1.3217e+00,  6.9309e-01,  5.4387e-01, -2.0573e+00, -1.4323e+00,\n",
      "          3.9829e-01,  2.9609e-01, -2.1869e-01, -1.0501e+00, -1.2297e+00,\n",
      "          5.8316e-01,  2.2196e-01,  2.8091e+00, -7.5279e-01,  1.3447e+00,\n",
      "          2.4429e-01,  2.7097e-01,  9.5317e-01,  1.5944e-02,  1.2455e+00,\n",
      "         -9.9247e-02,  3.7341e-01,  6.6829e-01,  1.4868e+00,  7.4454e-01,\n",
      "          2.1712e+00, -5.5992e-01,  2.6091e-01, -6.3272e-02, -8.1486e-01,\n",
      "         -1.1811e+00,  1.4681e-01, -1.0217e-01,  1.3490e+00, -1.2519e-02,\n",
      "          1.2997e-01,  1.1160e+00,  1.4374e-01,  8.6434e-01,  1.0258e+00,\n",
      "         -1.9600e-01, -4.1896e-02, -4.8240e-01,  5.8853e-01,  1.6379e-01,\n",
      "         -5.9955e-01, -5.6725e-01, -7.7617e-01,  2.3129e-01, -2.2512e-01,\n",
      "         -1.1146e+00,  5.2008e-01, -1.2717e+00,  3.7661e-01, -2.6566e-01,\n",
      "          9.6231e-01, -3.9602e-01, -8.1473e-01, -5.4216e-01, -6.2769e-03,\n",
      "         -6.3741e-01, -8.7859e-01, -5.8043e-01,  9.3087e-01, -2.6795e-01,\n",
      "          1.2149e+00, -1.1245e+00, -1.0048e+00],\n",
      "        [ 8.8929e-01,  1.2706e+00, -5.0707e-01,  1.6297e+00,  2.2532e+00,\n",
      "         -1.0448e-01,  5.1417e-01,  3.3458e-01,  3.5485e-01, -1.4447e+00,\n",
      "          3.2144e-01, -8.5007e-01, -1.4888e+00,  1.2608e+00,  2.6989e+00,\n",
      "          2.5848e-01,  1.3746e-01, -5.3252e-01,  1.7816e+00,  1.2122e+00,\n",
      "          3.5902e-02,  9.6671e-01,  1.2920e+00, -9.1846e-01,  8.2008e-01,\n",
      "         -1.1869e+00, -6.2326e-01, -3.0747e-01,  9.3115e-01, -5.2618e-01,\n",
      "          1.0618e-01,  2.6503e-01, -2.0154e-01, -2.0020e-01,  1.6004e+00,\n",
      "         -6.8864e-01,  2.2421e-01, -2.1880e+00,  3.0721e-01,  2.3164e-01,\n",
      "          5.4476e-01,  3.3234e-01, -1.1262e+00,  1.7455e+00,  7.9042e-02,\n",
      "         -9.1665e-01,  1.9266e+00, -1.0844e+00, -1.9829e+00, -3.2300e-01,\n",
      "         -1.5056e+00,  5.5248e-01, -1.2837e+00,  4.3387e-01,  8.5515e-01,\n",
      "          1.9461e-02, -1.0492e-01, -5.3472e-01,  2.2775e-01,  7.3188e-01,\n",
      "          1.3217e+00,  6.9309e-01,  5.4387e-01, -2.0573e+00, -1.4323e+00,\n",
      "          3.9829e-01,  2.9609e-01, -2.1869e-01, -1.0501e+00, -1.2297e+00,\n",
      "          5.8316e-01,  2.2196e-01,  2.8091e+00, -7.5279e-01,  1.3447e+00,\n",
      "          2.4429e-01,  2.7097e-01,  9.5317e-01,  1.5944e-02,  1.2455e+00,\n",
      "         -9.9247e-02,  3.7341e-01,  6.6829e-01,  1.4868e+00,  7.4454e-01,\n",
      "          2.1712e+00, -5.5992e-01,  2.6091e-01, -6.3272e-02, -8.1486e-01,\n",
      "         -1.1811e+00,  1.4681e-01, -1.0217e-01,  1.3490e+00, -1.2519e-02,\n",
      "          1.2997e-01,  1.1160e+00,  1.4374e-01,  8.6434e-01,  1.0258e+00,\n",
      "         -1.9600e-01, -4.1896e-02, -4.8240e-01,  5.8853e-01,  1.6379e-01,\n",
      "         -5.9955e-01, -5.6725e-01, -7.7617e-01,  2.3129e-01, -2.2512e-01,\n",
      "         -1.1146e+00,  5.2008e-01, -1.2717e+00,  3.7661e-01, -2.6566e-01,\n",
      "          9.6231e-01, -3.9602e-01, -8.1473e-01, -5.4216e-01, -6.2769e-03,\n",
      "         -6.3741e-01, -8.7859e-01, -5.8043e-01,  9.3087e-01, -2.6795e-01,\n",
      "          1.2149e+00, -1.1245e+00, -1.0048e+00],\n",
      "        [ 8.8929e-01,  1.2706e+00, -5.0707e-01,  1.6297e+00,  2.2532e+00,\n",
      "         -1.0448e-01,  5.1417e-01,  3.3458e-01,  3.5485e-01, -1.4447e+00,\n",
      "          3.2144e-01, -8.5007e-01, -1.4888e+00,  1.2608e+00,  2.6989e+00,\n",
      "          2.5848e-01,  1.3746e-01, -5.3252e-01,  1.7816e+00,  1.2122e+00,\n",
      "          3.5902e-02,  9.6671e-01,  1.2920e+00, -9.1846e-01,  8.2008e-01,\n",
      "         -1.1869e+00, -6.2326e-01, -3.0747e-01,  9.3115e-01, -5.2618e-01,\n",
      "          1.0618e-01,  2.6503e-01, -2.0154e-01, -2.0020e-01,  1.6004e+00,\n",
      "         -6.8864e-01,  2.2421e-01, -2.1880e+00,  3.0721e-01,  2.3164e-01,\n",
      "          5.4476e-01,  3.3234e-01, -1.1262e+00,  1.7455e+00,  7.9042e-02,\n",
      "         -9.1665e-01,  1.9266e+00, -1.0844e+00, -1.9829e+00, -3.2300e-01,\n",
      "         -1.5056e+00,  5.5248e-01, -1.2837e+00,  4.3387e-01,  8.5515e-01,\n",
      "          1.9461e-02, -1.0492e-01, -5.3472e-01,  2.2775e-01,  7.3188e-01,\n",
      "          1.3217e+00,  6.9309e-01,  5.4387e-01, -2.0573e+00, -1.4323e+00,\n",
      "          3.9829e-01,  2.9609e-01, -2.1869e-01, -1.0501e+00, -1.2297e+00,\n",
      "          5.8316e-01,  2.2196e-01,  2.8091e+00, -7.5279e-01,  1.3447e+00,\n",
      "          2.4429e-01,  2.7097e-01,  9.5317e-01,  1.5944e-02,  1.2455e+00,\n",
      "         -9.9247e-02,  3.7341e-01,  6.6829e-01,  1.4868e+00,  7.4454e-01,\n",
      "          2.1712e+00, -5.5992e-01,  2.6091e-01, -6.3272e-02, -8.1486e-01,\n",
      "         -1.1811e+00,  1.4681e-01, -1.0217e-01,  1.3490e+00, -1.2519e-02,\n",
      "          1.2997e-01,  1.1160e+00,  1.4374e-01,  8.6434e-01,  1.0258e+00,\n",
      "         -1.9600e-01, -4.1896e-02, -4.8240e-01,  5.8853e-01,  1.6379e-01,\n",
      "         -5.9955e-01, -5.6725e-01, -7.7617e-01,  2.3129e-01, -2.2512e-01,\n",
      "         -1.1146e+00,  5.2008e-01, -1.2717e+00,  3.7661e-01, -2.6566e-01,\n",
      "          9.6231e-01, -3.9602e-01, -8.1473e-01, -5.4216e-01, -6.2769e-03,\n",
      "         -6.3741e-01, -8.7859e-01, -5.8043e-01,  9.3087e-01, -2.6795e-01,\n",
      "          1.2149e+00, -1.1245e+00, -1.0048e+00],\n",
      "        [-4.1344e-01,  1.2543e+00, -1.4747e-01,  9.1159e-02, -8.0947e-01,\n",
      "          8.8803e-01,  1.2867e+00,  1.0703e+00, -7.8164e-01,  1.0913e+00,\n",
      "         -5.2578e-04,  3.3823e-01, -6.0736e-01,  5.9365e-01,  2.0960e-01,\n",
      "          4.3781e-01, -1.3259e+00, -4.7955e-01,  1.9710e+00, -1.0748e+00,\n",
      "         -3.4127e-01, -1.2070e+00, -1.5072e+00,  1.9717e-01, -1.6732e+00,\n",
      "          3.4056e-01,  3.2793e-01,  3.3916e-01,  5.0932e-01, -5.3167e-01,\n",
      "          2.1647e+00, -8.9505e-02,  7.5556e-01,  5.5764e-01,  7.8123e-02,\n",
      "         -4.9313e-01, -2.1219e-01, -2.6179e+00,  7.1335e-01, -4.4037e-01,\n",
      "          1.2163e+00,  1.3838e-01,  6.8905e-01, -4.2152e-01, -9.9513e-01,\n",
      "          7.5093e-02, -1.2819e-01, -2.4233e+00,  1.8951e-01,  5.0495e-02,\n",
      "          1.7387e+00,  1.0625e+00,  6.2764e-02, -5.5292e-01, -1.2912e+00,\n",
      "         -9.8051e-01,  6.2482e-01,  8.3605e-01,  2.6120e-01,  7.0925e-01,\n",
      "          3.7053e-01,  3.7959e-01,  2.2844e-01,  1.2796e-01,  8.5408e-01,\n",
      "          5.9722e-02, -1.0256e+00,  1.3840e-01,  4.9791e-02,  7.3646e-01,\n",
      "         -4.6400e-01,  1.1702e+00, -1.8769e+00,  5.2367e-01, -5.9559e-01,\n",
      "          9.8249e-01, -6.5355e-01,  8.2804e-01, -5.3873e-01,  1.3629e+00,\n",
      "          4.8778e-01, -2.2882e+00, -4.2967e-02,  4.6473e-01, -1.3184e+00,\n",
      "         -1.6972e+00,  8.7936e-01,  9.4923e-01, -6.3015e-01, -2.3233e-01,\n",
      "         -9.9068e-01, -1.2788e-01, -5.1511e-01,  2.2914e+00, -1.8853e-01,\n",
      "          1.2466e+00, -9.9640e-01,  2.0091e-02,  2.2609e-01, -9.0990e-01,\n",
      "         -2.2636e+00, -6.7189e-02, -4.8426e-01,  5.0136e-01, -3.7588e-01,\n",
      "         -1.3101e-02,  2.0002e+00, -1.5918e-01,  1.9779e-01, -5.7440e-02,\n",
      "          1.7416e-01, -6.3891e-01,  1.6813e+00, -1.0384e+00, -2.8906e-02,\n",
      "          1.5332e-01,  6.6823e-01,  2.3306e-02, -7.5043e-01, -8.1005e-01,\n",
      "         -1.0561e+00, -9.4934e-01,  1.2379e+00, -3.5240e-01, -2.4485e-01,\n",
      "         -1.3544e+00,  3.6426e-01,  1.8487e+00],\n",
      "        [ 8.8929e-01,  1.2706e+00, -5.0707e-01,  1.6297e+00,  2.2532e+00,\n",
      "         -1.0448e-01,  5.1417e-01,  3.3458e-01,  3.5485e-01, -1.4447e+00,\n",
      "          3.2144e-01, -8.5007e-01, -1.4888e+00,  1.2608e+00,  2.6989e+00,\n",
      "          2.5848e-01,  1.3746e-01, -5.3252e-01,  1.7816e+00,  1.2122e+00,\n",
      "          3.5902e-02,  9.6671e-01,  1.2920e+00, -9.1846e-01,  8.2008e-01,\n",
      "         -1.1869e+00, -6.2326e-01, -3.0747e-01,  9.3115e-01, -5.2618e-01,\n",
      "          1.0618e-01,  2.6503e-01, -2.0154e-01, -2.0020e-01,  1.6004e+00,\n",
      "         -6.8864e-01,  2.2421e-01, -2.1880e+00,  3.0721e-01,  2.3164e-01,\n",
      "          5.4476e-01,  3.3234e-01, -1.1262e+00,  1.7455e+00,  7.9042e-02,\n",
      "         -9.1665e-01,  1.9266e+00, -1.0844e+00, -1.9829e+00, -3.2300e-01,\n",
      "         -1.5056e+00,  5.5248e-01, -1.2837e+00,  4.3387e-01,  8.5515e-01,\n",
      "          1.9461e-02, -1.0492e-01, -5.3472e-01,  2.2775e-01,  7.3188e-01,\n",
      "          1.3217e+00,  6.9309e-01,  5.4387e-01, -2.0573e+00, -1.4323e+00,\n",
      "          3.9829e-01,  2.9609e-01, -2.1869e-01, -1.0501e+00, -1.2297e+00,\n",
      "          5.8316e-01,  2.2196e-01,  2.8091e+00, -7.5279e-01,  1.3447e+00,\n",
      "          2.4429e-01,  2.7097e-01,  9.5317e-01,  1.5944e-02,  1.2455e+00,\n",
      "         -9.9247e-02,  3.7341e-01,  6.6829e-01,  1.4868e+00,  7.4454e-01,\n",
      "          2.1712e+00, -5.5992e-01,  2.6091e-01, -6.3272e-02, -8.1486e-01,\n",
      "         -1.1811e+00,  1.4681e-01, -1.0217e-01,  1.3490e+00, -1.2519e-02,\n",
      "          1.2997e-01,  1.1160e+00,  1.4374e-01,  8.6434e-01,  1.0258e+00,\n",
      "         -1.9600e-01, -4.1896e-02, -4.8240e-01,  5.8853e-01,  1.6379e-01,\n",
      "         -5.9955e-01, -5.6725e-01, -7.7617e-01,  2.3129e-01, -2.2512e-01,\n",
      "         -1.1146e+00,  5.2008e-01, -1.2717e+00,  3.7661e-01, -2.6566e-01,\n",
      "          9.6231e-01, -3.9602e-01, -8.1473e-01, -5.4216e-01, -6.2769e-03,\n",
      "         -6.3741e-01, -8.7859e-01, -5.8043e-01,  9.3087e-01, -2.6795e-01,\n",
      "          1.2149e+00, -1.1245e+00, -1.0048e+00],\n",
      "        [ 3.1657e-01,  1.6857e+00, -7.5256e-01, -2.7626e-01,  1.3170e+00,\n",
      "         -1.9299e+00,  1.2743e+00,  5.1968e-01,  1.7429e+00, -5.1237e-02,\n",
      "         -5.3475e-01, -8.8675e-01, -3.7914e-01,  7.2053e-02, -8.6813e-01,\n",
      "          6.1450e-02, -4.6709e-01, -1.1848e-01,  4.0260e-01, -6.2226e-01,\n",
      "          5.2668e-01,  1.8195e-01,  6.7055e-04,  1.9897e-01, -6.1843e-01,\n",
      "          2.3569e+00, -1.5629e+00,  2.3364e-01,  1.1173e+00, -1.6525e-01,\n",
      "         -3.3337e-03, -6.7812e-01, -8.7704e-01,  1.9425e-01, -8.8169e-01,\n",
      "          8.5786e-01,  3.0313e-01, -2.4733e-01,  2.6018e-01, -2.7064e-01,\n",
      "         -1.7342e-01,  1.0856e-01, -2.0228e+00, -5.7603e-01,  1.1725e+00,\n",
      "          1.4054e+00,  1.8201e+00,  1.0453e+00,  6.6859e-01,  8.6837e-02,\n",
      "          1.0815e+00, -9.7243e-02,  2.6639e-01, -4.4841e-01,  1.0523e+00,\n",
      "         -5.8951e-02, -4.0759e-01, -5.7835e-01,  1.7914e+00, -3.4853e-02,\n",
      "          8.7528e-01,  1.5106e+00,  2.4559e-01,  1.3043e-01, -6.4887e-01,\n",
      "         -8.9293e-01,  1.0158e+00,  5.4671e-01,  2.5138e-01,  1.6714e+00,\n",
      "         -6.2035e-02, -3.3120e-01, -1.2367e+00, -4.7145e-01,  9.9232e-01,\n",
      "         -8.5448e-01,  1.8682e-01, -4.6673e-01,  7.5442e-01,  1.0755e+00,\n",
      "          2.5201e-01,  6.3706e-01, -1.5126e+00,  6.7960e-01,  1.1765e+00,\n",
      "         -1.3894e+00,  6.3628e-01,  1.4608e-01, -2.1677e+00, -5.9428e-01,\n",
      "         -6.6957e-01, -1.6534e+00,  6.3664e-02,  1.5471e-01,  3.5065e-01,\n",
      "          4.4446e-01,  9.0899e-01,  1.0500e+00,  2.4119e+00, -1.3916e+00,\n",
      "         -5.8181e-01,  5.5110e-01, -8.6889e-01, -8.2323e-01, -1.0617e+00,\n",
      "         -3.5876e-01, -1.9555e+00, -2.1495e+00,  7.2453e-01,  1.1540e+00,\n",
      "         -6.6650e-01, -6.0845e-01, -5.0093e-02,  5.7081e-01, -5.1796e-02,\n",
      "         -2.0524e-01,  4.5505e-01,  2.7093e-01, -1.9624e+00, -8.0041e-01,\n",
      "         -1.2548e+00,  6.7898e-01,  6.4687e-01, -1.8640e+00, -2.8332e-01,\n",
      "         -3.1038e-01,  3.2671e-01, -1.0723e+00]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = length\n",
    "out_dim = 128\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, out_dim)\n",
    "\n",
    "token_embedding = token_embedding_layer(torch.tensor(encoded_stc))\n",
    "\n",
    "print(token_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e8f78",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2 position embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1404b1",
   "metadata": {},
   "source": [
    "Just now, we implement the token embedding. And now we must to embed its position. In practice, there are two ways, such as **Learned Positional Embedding**, **Sinusoidal Positional Embedding** and **Rotary Position Embedding** also called **RoPE**\n",
    "\n",
    "In this section, we talk about all of the three embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6a552",
   "metadata": {},
   "source": [
    "---\n",
    "### 2.1 Learned Position Embedding\n",
    "\n",
    "This is the most simple position embedding, and it's implemention is also simple.\n",
    "\n",
    "First, we initialize a group of learnable vectors.\n",
    "\n",
    "Unlike token embedding recieve `vocab_size` and embed it to `out_dim`. Position embedding should recieve the `context_size` and embed it to `out_dim` for we are embedding the position. Here we must note: the token embedding's `out_dim` must be the same with position embedding's `out_dim`. Because in the later we will plus them. And the operated one is finally what we want. It contains the semantics also the position information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47ba24c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 4\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, out_dim)\n",
    "\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80512c",
   "metadata": {},
   "source": [
    "Then we plus the token_embedding and pos_embedding. It's the final embedding of the `input`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f58c065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: The size of tensor a (7) must match the size of tensor b (4) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    input_embedding = token_embedding + pos_embedding\n",
    "    print(input_embedding)\n",
    "except Exception as e:\n",
    "    print(\"error:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384a4e5",
   "metadata": {},
   "source": [
    "F***, why there make a mistake? Encounting this, we must have to learn to read the error information. It said: the tensor don't match in the first dimension. And we have a check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b60bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_embedding shape: torch.Size([7, 128])\n",
      "pos_embedding shape: torch.Size([4, 128])\n"
     ]
    }
   ],
   "source": [
    "print(\"token_embedding shape:\", token_embedding.shape)\n",
    "print(\"pos_embedding shape:\", pos_embedding.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d62b73",
   "metadata": {},
   "source": [
    "And we see their shape don't match. Here actually casued for the `context_size`, we set it to 4. But the stence is not 4. Here I play a little joke. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5677e24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4802, -1.0503, -1.6657, -0.9118, -0.2137, -0.4465, -0.3929,  1.7841,\n",
      "          0.3638,  0.7323, -0.8227, -1.7517,  0.9578, -4.7607,  1.1408,  0.4597,\n",
      "          2.1275, -0.1138, -2.9951, -2.5843,  0.6740, -0.9421, -0.9980,  1.5200,\n",
      "         -0.5968, -0.3624,  0.8909, -0.6683, -0.0524, -1.9236, -2.0040, -0.4103,\n",
      "          0.8952,  0.7189, -1.4367,  0.9294,  2.4446,  1.5465, -2.5476,  0.8322,\n",
      "          1.3615, -0.9421,  2.6232,  0.2398,  0.0478, -1.3550,  0.4974, -1.3270,\n",
      "         -0.8104, -2.0113, -1.9560,  1.0423, -0.2453,  1.7721,  1.9592,  1.3576,\n",
      "          0.9166,  0.4446,  0.3640,  1.6002, -0.2627, -3.0990, -0.5889, -3.1357,\n",
      "          0.4318, -1.1536,  0.7579,  0.0335, -0.3818,  2.9200,  1.2804,  2.4870,\n",
      "          2.8983, -2.7241,  1.0430,  1.0653,  0.8239, -0.1541,  0.4653, -1.6424,\n",
      "          1.3140, -0.8844, -0.1992,  0.4843,  2.8931, -0.2785,  0.4063,  1.3585,\n",
      "          2.1602,  1.4551, -1.9558, -1.7038,  1.9775, -0.6636,  0.3163, -0.1401,\n",
      "          1.4877,  0.5716,  2.0944, -0.9503,  0.1755, -0.2341, -0.4646,  0.5229,\n",
      "          0.3982,  1.3309, -0.4687,  1.1077,  0.1061,  0.3608, -0.5841, -1.1388,\n",
      "          0.7759, -0.4172, -0.0780,  1.1149, -0.4563, -0.8802,  0.5728,  1.8331,\n",
      "         -1.8164, -1.6502, -0.3686,  2.2255,  0.0814,  0.5061,  0.5235,  0.0687],\n",
      "        [ 1.1496,  0.2318, -0.8483,  0.6079,  0.7177, -0.6129,  1.2033,  0.6930,\n",
      "          0.1179, -1.5958, -0.7584,  1.8523,  0.5180,  2.0304,  1.3983,  1.0676,\n",
      "          1.4218, -1.3481,  3.1721,  2.2233, -0.3574,  1.1492,  0.6838, -0.3707,\n",
      "          0.7708, -1.2253, -1.6037,  1.0617,  1.5680,  0.4606,  1.3666,  1.0374,\n",
      "         -2.1398,  0.5277,  1.1065, -1.6691,  2.1853, -0.1798, -0.0782, -1.3967,\n",
      "         -1.6175,  0.3081, -0.9412,  0.7902, -0.3770, -1.4189,  1.1150, -0.2273,\n",
      "         -2.6380, -0.0087, -2.8874, -0.5372, -2.4630,  0.1874,  0.3429,  0.4028,\n",
      "          0.9892, -0.5031,  1.7507, -0.0267,  0.9551,  0.2146,  0.3103, -3.4070,\n",
      "         -1.3496,  1.2554,  0.3879, -1.5369, -0.3990, -0.1064,  0.4801,  1.0922,\n",
      "          3.6010, -1.2315,  0.5637,  1.3340, -0.4786, -1.0280, -0.0366,  2.1500,\n",
      "         -1.4508,  0.1925,  1.7901,  0.8465,  0.8393,  1.6149, -2.2891,  1.4990,\n",
      "         -0.2921, -1.7511, -0.7316,  0.2456,  0.0888,  0.8673,  0.9894,  0.5863,\n",
      "          1.2143,  0.5752,  1.9977,  0.9174, -0.6711,  0.6571, -0.9555,  0.5125,\n",
      "          1.6459,  0.5155, -1.1590, -1.5146,  1.5630,  0.6152, -0.9291,  1.2683,\n",
      "         -1.2073,  0.5829,  1.2768,  0.6664, -2.4723, -2.4231, -0.2872,  0.1475,\n",
      "          0.6818, -1.7104,  0.9486,  0.3620,  1.0470,  1.0756, -1.5549, -0.0908],\n",
      "        [ 1.7134,  2.7340, -0.8483,  3.2736,  2.4292,  0.3511,  0.2168, -0.5580,\n",
      "          0.1829, -2.3816, -0.0401,  0.0149, -1.4169,  0.7266,  2.5044,  0.6307,\n",
      "         -0.4103, -0.8608,  1.2013,  0.4712, -0.1075,  0.6002,  0.8497, -1.4100,\n",
      "          0.4791, -0.2369,  1.4460, -2.1546,  0.5781, -0.8435,  0.2850,  2.4207,\n",
      "         -0.4950,  1.2980,  2.5140, -0.5085, -0.4796, -3.0348, -0.0656,  1.1617,\n",
      "         -0.2831, -0.2526,  0.4110,  4.4478,  0.1151, -2.2331,  0.6411, -1.9801,\n",
      "         -3.4137, -0.0488, -0.1654,  1.8391, -2.0806,  1.4901,  1.5614,  1.2009,\n",
      "         -1.3836, -1.1893,  1.3535,  2.7985,  1.0995, -0.7027,  0.1463, -3.9163,\n",
      "         -1.8069, -0.5250,  0.9995, -0.9106, -0.0270, -1.4815,  1.1061, -0.1378,\n",
      "          4.2695,  1.6134,  0.0639,  1.7716, -0.1406,  0.5403, -0.1449,  0.8367,\n",
      "         -0.5552,  1.2202,  0.4863,  1.7565,  1.0553,  3.2250, -0.9187,  1.6869,\n",
      "          0.9330, -0.9459, -2.8730, -0.5367,  0.5393,  0.7060,  1.2192,  0.8193,\n",
      "          0.0391, -0.8233,  1.2325, -0.7534, -0.1470, -0.4754,  0.8234, -0.7312,\n",
      "          1.1134, -0.3141, -0.2630, -0.8431,  0.6950,  0.3227, -0.3542,  1.9548,\n",
      "         -2.2785,  2.0729,  0.4575,  0.5444,  0.4233, -2.4575, -1.4347, -0.1424,\n",
      "          0.1829, -0.0809, -0.2315, -0.4628,  0.8052,  0.8588, -2.5945, -0.9815],\n",
      "        [ 0.1283,  1.8109, -0.4024,  2.3497,  3.0990,  0.5395, -2.0069,  0.1592,\n",
      "          1.2731, -0.3994,  2.0157, -1.4255, -1.0552,  0.6299,  2.2470,  1.9149,\n",
      "          0.0725, -1.1895,  0.9877,  1.0475, -0.2856,  0.3127,  2.2763, -0.0225,\n",
      "          1.7178, -1.0202,  0.7164, -1.9184,  1.3817, -0.6363,  0.4521,  0.3927,\n",
      "          1.5391, -0.3907,  2.4128, -0.2311,  0.5161, -1.1762,  0.4155,  1.1196,\n",
      "          2.5965,  2.1459, -0.5941,  1.7951, -1.1738, -0.1985,  3.4170, -1.2319,\n",
      "          0.9875,  1.3833, -3.2912, -0.3353, -0.6821, -0.2298,  1.3441, -0.4728,\n",
      "         -0.4775,  0.1182, -0.4197,  1.7773, -0.0897,  2.4674,  0.4147, -2.4326,\n",
      "         -0.7537, -0.6791,  1.2105,  3.1394,  1.0253, -1.6581,  0.0299,  0.3335,\n",
      "          2.2736, -2.4478,  1.7234,  0.0077,  1.7840,  2.1261,  0.1082,  1.7419,\n",
      "         -1.0180,  1.0925,  2.6753,  1.4238, -0.9872,  1.9656, -2.5929,  0.1535,\n",
      "         -0.4614, -2.2573, -2.6876,  1.4741,  2.2440,  1.2175, -0.7336, -0.7529,\n",
      "          1.1335,  1.4261,  0.9228, -0.1077,  0.5977,  1.7587, -1.5049,  0.9990,\n",
      "         -0.2745, -0.5856, -1.1120,  1.2426, -0.0604, -1.0586, -1.3987,  1.4316,\n",
      "         -2.5913,  1.5132, -1.1019,  0.8343, -0.3490, -1.9440, -0.4442,  0.6114,\n",
      "         -2.2378, -0.9883, -0.2362,  1.0812, -0.9908, -0.7723, -1.3984, -0.7200],\n",
      "        [-0.7537,  1.5479, -0.2022,  0.7978, -1.4152,  1.8548,  2.7831,  4.3052,\n",
      "         -2.0071,  1.7789,  0.2637, -1.6508, -0.3015, -0.6292,  0.7175, -0.0775,\n",
      "         -2.5831, -0.1515,  2.4839, -0.7270, -1.0185, -0.9464, -2.1079,  1.2068,\n",
      "          0.2942,  0.0439, -1.1830, -0.5616,  1.1692, -1.2325, -0.0981, -0.1932,\n",
      "          1.0877,  0.4829,  1.6051,  0.2446,  0.2381, -3.3268,  0.4348,  0.6185,\n",
      "          0.8840,  0.5266,  1.2975, -0.9455, -0.3072, -0.5173, -0.8778, -3.5890,\n",
      "         -1.5008,  0.0896,  1.0895,  1.1137,  0.6366, -2.4511, -0.5086, -0.0336,\n",
      "          2.6185,  0.6911,  0.4309, -0.4035,  0.0966,  2.2709, -0.0710, -0.9908,\n",
      "          2.0766, -0.9755, -1.7126,  0.0304, -0.2657,  0.3217, -1.3240,  0.3871,\n",
      "         -1.6839, -0.4258, -2.0193,  3.0758, -0.5168, -1.4458, -2.0767,  1.6439,\n",
      "         -0.4069, -2.5249,  0.8255,  0.9600, -0.9579, -1.0456, -0.3333,  1.5165,\n",
      "         -2.7568,  0.5450,  0.3070, -1.3469, -1.8994,  1.9839,  0.3658, -0.6473,\n",
      "         -2.0575, -1.7198,  2.5724, -3.0855, -0.6963,  0.0126, -0.3736,  0.1149,\n",
      "         -0.7054,  1.4608,  2.6549,  0.8128, -0.7476,  0.4401,  0.6028, -0.2492,\n",
      "          1.7733, -1.7880, -0.9446,  0.5816,  0.8899, -0.0852, -0.3841, -0.5961,\n",
      "         -0.8257, -0.0857, -0.0133, -1.2913, -1.9324, -1.1126,  0.7917,  2.3302],\n",
      "        [ 1.2863,  1.0080,  0.8633,  4.2981,  1.6148, -1.7975,  0.1822,  1.0565,\n",
      "         -0.0138, -2.0297, -1.5228, -1.9585, -1.9361,  1.0070,  4.1681,  0.0880,\n",
      "          1.2089, -0.4492,  0.7478,  0.5312,  0.9136,  1.6959, -0.0892,  0.0826,\n",
      "          0.3636, -2.0706, -0.3981, -1.2728, -0.3369, -1.6871,  2.2551,  0.1109,\n",
      "          0.0421, -0.1968,  2.1057, -1.0395, -1.5596, -1.6794, -0.9512,  1.4345,\n",
      "          0.4127,  1.0470, -2.1689,  2.0539,  1.9850,  0.2174,  2.6198, -0.6301,\n",
      "         -1.8293, -1.6689, -1.4488,  1.0476, -0.9670, -0.6185,  0.5005, -1.6349,\n",
      "          0.3059,  3.0873,  0.7121,  1.3278,  0.9772,  1.1096,  0.5943, -1.7362,\n",
      "         -3.2403, -0.5648,  0.7354,  0.2214, -4.3741, -3.3508,  0.2763, -0.0763,\n",
      "          3.2377, -1.2692,  2.9119, -0.6985,  0.7031,  1.5818,  1.3870,  1.4423,\n",
      "         -2.3804, -0.0171, -0.3595,  1.1431,  0.9422,  2.0593, -1.0440,  1.5367,\n",
      "         -0.2251, -2.0000, -0.6253, -0.6034, -0.4825,  0.6684, -3.1107,  0.4971,\n",
      "          0.5360,  3.2269, -0.7232, -0.0053,  0.0575, -2.0662,  0.2118,  1.2263,\n",
      "          0.4022, -1.2543, -0.1848,  0.1285,  0.3099, -0.3226, -0.8411,  0.4087,\n",
      "         -2.2782, -0.5947, -1.1238,  1.0923, -1.6643, -0.3514,  0.4906,  0.9970,\n",
      "         -1.1720, -0.4776, -0.2522,  1.4229, -1.5957,  2.2164, -1.4142, -1.8487],\n",
      "        [ 0.1502,  2.9621, -1.0365, -0.8882,  1.0092, -0.8077,  1.6025, -1.4235,\n",
      "          2.2298,  1.1580, -0.5910, -0.3435, -0.6193,  1.2990,  0.0920, -0.3676,\n",
      "         -1.5939, -1.5012,  0.7418, -2.4271, -2.1031,  0.7761, -0.3766,  1.7449,\n",
      "          0.1573,  2.7129, -2.0311,  1.2310,  0.4497, -0.3306, -0.0459,  0.0879,\n",
      "         -0.4116, -1.7603, -0.7842,  0.3796,  1.0880,  0.1775,  0.1594, -0.0737,\n",
      "          0.9276,  1.3540, -3.3336, -1.3270,  2.3709,  1.8049,  2.8647,  0.5903,\n",
      "          1.3434,  0.7206,  1.8699,  1.1686,  0.2910, -1.0709,  2.0018,  0.9237,\n",
      "         -0.2721, -0.0968,  0.6302,  1.0231,  0.2652,  2.7493, -0.3471,  0.6210,\n",
      "         -0.9512,  0.4608,  0.5527, -0.0880,  0.7125,  1.3743,  0.3716,  0.5662,\n",
      "         -2.1096, -0.8874,  1.3082, -0.4101,  1.4147,  1.4876,  2.1134,  1.5163,\n",
      "         -0.7493, -1.1290, -2.5454,  1.3289,  0.4197, -0.2129,  0.5557,  0.1392,\n",
      "         -1.6890, -0.3576, -0.6025, -1.1397,  1.2011,  1.5670,  0.4108,  2.5967,\n",
      "          0.1392,  1.9690,  4.4271, -1.1108, -0.5868,  0.8873, -1.6732, -1.0113,\n",
      "         -1.3580, -1.5240, -2.4222, -2.8437,  0.0110,  2.3470,  0.5898,  0.6915,\n",
      "          0.4398,  1.0139,  0.5842,  0.6076,  0.4536, -1.1407, -1.2072, -1.6328,\n",
      "         -0.2515,  0.6520,  1.7147, -1.5996, -0.5980,  0.6548, -0.0405, -1.9918]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_size = 7\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, out_dim)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_size))\n",
    "\n",
    "input_embedding = token_embedding + pos_embedding\n",
    "\n",
    "print(input_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9c7d9b",
   "metadata": {},
   "source": [
    "But there a question: what if another sentence come with different size? Then we should build another embeddingm and if we continously recieve sentence, our memory will overflow. So we should introduce some method to partition long sentence to fixed length piece. There we use `Dataset` and `DataLoader`. If you don't know what this is, This is [Dataset](../basic_pytorch/data/Dataset/Dataset.ipynb) and this is [DataLoader](../basic_pytorch/data/DataLoader/DataLoader.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0410f20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDatset(Dataset):\n",
    "    \n",
    "    # instantiate function\n",
    "    def __init__(self, text, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(text)\n",
    "        assert len(token_ids) > max_len, \"Number of tokenized inputs must at %max_len + 1\" % max_len\n",
    "\n",
    "        for i in range(0, len(token_ids) - max_len, stride):\n",
    "            input_chunk = token_ids[i : i + max_len]\n",
    "            target_chunk = token_ids[i + 1 : i + 1 + max_len]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ae932",
   "metadata": {},
   "source": [
    "Have the dataset, then we can use PyTorch's DataLoader to automatically generate data for us. It will call the Dataset's `__getitem__` method to return datas for us.\n",
    "\n",
    "Here we don't use our own tokenizer. That means we don't use the corpus of that English poem. We use pre-trained tokenizer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6238aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0e0f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(\n",
    "        text, batch_size=4, max_len=128, stride=4,\n",
    "        shuffle=True, drop_last=True, num_workers=0,\n",
    "        model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\", cache_dir=\"../tokenizer/deepseek\"\n",
    "):\n",
    "    # Initialize pre-trained tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    \n",
    "    # Create dataset, using our CustomDataset class\n",
    "    dataset = CustomDatset(text, tokenizer, max_len, stride)\n",
    "\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14aebb",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99863b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can I hold you with?\n",
      "\n",
      "I offer you lean streets, desperate sunsets, the moon of the jagged suburbs.\n",
      "\n",
      "I offer you the bitterness of a man who has looked long and long at the lonely moon.\n",
      "\n",
      "I offer you my ancestors, my dead men, the ghosts that living men have honoured in bronze: my father's father killed in the frontier of Buenos Aires, two bullets through his lungs, bearded and dead, wrapped by his soldiers in the hide of a cow; my mother's grandfather --just twentyfour--heading a charge of three hundred men in Peru, now ghosts on vanished horses.\n",
      "\n",
      "I offer you whatever insight my books may hold, whatever manliness or humour my life.\n",
      "\n",
      "I offer you the loyalty of a man who has never been loyal.\n",
      "\n",
      "I offer you that kernel of myself that I have saved, somehow --the central heart that deals not in words, traffics not with dreams, and is untouched by time, by joy, by adversities.\n",
      "\n",
      "I offer you the memory of a yellow rose seen at sunset, years before you were born.\n",
      "\n",
      "I offer you explanations of yourself, theories about yourself, authentic and surprising news of yourself.\n",
      "\n",
      "I can give you my loneliness, my darkness, the hunger of my heart; I am trying to bribe you with uncertainty, with danger, with defeat.\n"
     ]
    }
   ],
   "source": [
    "with open(\"../corpus/poetry.txt\", \"r\") as file:\n",
    "    raw_text = file.read()\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6e657d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[151646,   3838,    646,    358]]), tensor([[3838,  646,  358, 3331]])]\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=1, max_len=4, stride=1, shuffle=False\n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c3d0598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[3838,  646,  358, 3331]]), tensor([[ 646,  358, 3331,  498]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef890a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[151646,   3838,    646,    358],\n",
      "        [  3331,    498,    448,   1939],\n",
      "        [    40,   3010,    498,  15651],\n",
      "        [ 14371,     11,  27395,   7015],\n",
      "        [  4917,     11,    279,  17788],\n",
      "        [   315,    279,  26742,   3556],\n",
      "        [ 46913,    382,     40,   3010],\n",
      "        [   498,    279,  78996,    315]])\n",
      "\n",
      "targets:\n",
      " tensor([[ 3838,   646,   358,  3331],\n",
      "        [  498,   448,  1939,    40],\n",
      "        [ 3010,   498, 15651, 14371],\n",
      "        [   11, 27395,  7015,  4917],\n",
      "        [   11,   279, 17788,   315],\n",
      "        [  279, 26742,  3556, 46913],\n",
      "        [  382,    40,  3010,   498],\n",
      "        [  279, 78996,   315,   264]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader(\n",
    "    raw_text, batch_size=8, max_len=4, stride=4, shuffle=False \n",
    ")\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\ntargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd7503",
   "metadata": {},
   "source": [
    "So, we can use the format data, they all have the same length. So they don't need to use so many Embedding, instead one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42afc4f",
   "metadata": {},
   "source": [
    "Cause we have replace our tokenizer to the pretrained one. So it's vocab changed. We must use the pretrained's vocab, or we will encounter error of out of index for the tokenizered id exceeds our own vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2389d8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "token_embedding_layer shape: torch.Size([151665, 4])\n",
      "token_embedding_layer: Parameter containing:\n",
      "tensor([[ 0.7939,  0.8734,  0.6884,  1.7565],\n",
      "        [-1.5711,  0.4150,  1.3614, -0.6327],\n",
      "        [-1.4541,  2.1737,  0.5489,  0.5346],\n",
      "        ...,\n",
      "        [ 0.0847,  0.3511, -0.0932, -0.6103],\n",
      "        [-0.6879,  1.9135,  0.4878,  0.7240],\n",
      "        [ 1.4361, -0.9227, -0.8750,  1.1611]], requires_grad=True)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pos_embedding_layer shape: torch.Size([4, 4])\n",
      "pos_embedding_layer: Parameter containing:\n",
      "tensor([[-0.7671, -0.1381, -1.2786,  0.1141],\n",
      "        [-0.1188, -1.3545, -1.5097,  0.0086],\n",
      "        [-1.2312, -0.3264, -1.4235, -0.1373],\n",
      "        [-0.4512,  2.0651,  0.7018, -0.4913]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# get the vocab's size\n",
    "# first we initialize the tokenizer\n",
    "model_name=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "cache_dir=\"../tokenizer/deepseek\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "vocab_size = len(tokenizer.get_vocab())\n",
    "out_dim = 4\n",
    "context_size = 4\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, out_dim)\n",
    "\n",
    "pos_embedding_layer = torch.nn.Embedding(context_size, out_dim)\n",
    "\n",
    "print(\"-\"*100)\n",
    "print(\"token_embedding_layer shape:\", token_embedding_layer.weight.shape)\n",
    "print(\"token_embedding_layer:\", token_embedding_layer.weight)\n",
    "print(\"-\"*100)\n",
    "print(\"pos_embedding_layer shape:\", pos_embedding_layer.weight.shape)\n",
    "print(\"pos_embedding_layer:\", pos_embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cef5a5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "token_embedding shape: torch.Size([8, 4, 4])\n",
      "token_embedding:\n",
      " tensor([[[ 1.6524, -0.5745,  0.4926, -0.0387],\n",
      "         [-0.7643, -0.6054, -3.2532, -0.1840],\n",
      "         [ 0.1903,  0.3190,  0.4248,  0.8736],\n",
      "         [ 0.8863, -0.0117, -0.8368,  0.9156]],\n",
      "\n",
      "        [[-1.1193,  0.8943, -0.7088, -0.6777],\n",
      "         [-0.5820,  2.1907, -1.4669, -1.0537],\n",
      "         [ 0.9073,  0.4651, -0.6132, -0.7197],\n",
      "         [ 0.5991,  2.3032,  0.7870,  0.5889]],\n",
      "\n",
      "        [[-0.7504,  0.9921,  1.7085,  0.5891],\n",
      "         [-0.1655, -0.8471, -0.2278,  0.7334],\n",
      "         [-0.5820,  2.1907, -1.4669, -1.0537],\n",
      "         [ 1.4607, -0.4790, -1.2052,  0.0586]],\n",
      "\n",
      "        [[ 0.3028, -1.1045,  0.2325, -0.3734],\n",
      "         [ 0.5084, -0.7684, -1.5536,  0.7362],\n",
      "         [ 0.8154,  1.1250,  0.7514, -0.5589],\n",
      "         [-0.3895,  1.0679,  1.5536, -1.5172]],\n",
      "\n",
      "        [[-0.5075, -0.1102,  0.3038, -1.1680],\n",
      "         [ 0.5084, -0.7684, -1.5536,  0.7362],\n",
      "         [-1.1344,  0.8358,  0.7060, -0.2742],\n",
      "         [-0.0178,  0.5419,  1.5013, -0.8167]],\n",
      "\n",
      "        [[ 1.5657,  0.6969,  2.2021, -0.3776],\n",
      "         [-1.1344,  0.8358,  0.7060, -0.2742],\n",
      "         [-0.3211, -0.2937,  0.0735,  0.7471],\n",
      "         [-0.7588,  0.8875,  0.2923, -0.3309]],\n",
      "\n",
      "        [[ 0.8164,  0.3786,  0.1830,  1.2348],\n",
      "         [-1.2001, -0.0344, -2.1395, -2.6030],\n",
      "         [-0.7504,  0.9921,  1.7085,  0.5891],\n",
      "         [-0.1655, -0.8471, -0.2278,  0.7334]],\n",
      "\n",
      "        [[-0.5820,  2.1907, -1.4669, -1.0537],\n",
      "         [-1.1344,  0.8358,  0.7060, -0.2742],\n",
      "         [-0.6162, -1.8383, -0.2539, -0.5102],\n",
      "         [ 1.5657,  0.6969,  2.2021, -0.3776]]], grad_fn=<EmbeddingBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "pos_embedding shape: torch.Size([4, 4])\n",
      "pos_embedding:\n",
      " tensor([[-0.7671, -0.1381, -1.2786,  0.1141],\n",
      "        [-0.1188, -1.3545, -1.5097,  0.0086],\n",
      "        [-1.2312, -0.3264, -1.4235, -0.1373],\n",
      "        [-0.4512,  2.0651,  0.7018, -0.4913]], grad_fn=<EmbeddingBackward0>)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "inpu_embedding shape: torch.Size([8, 4, 4])\n",
      "input embedding:\n",
      " tensor([[[ 0.8854, -0.7125, -0.7860,  0.0754],\n",
      "         [-0.8831, -1.9599, -4.7629, -0.1754],\n",
      "         [-1.0409, -0.0075, -0.9988,  0.7362],\n",
      "         [ 0.4351,  2.0533, -0.1350,  0.4243]],\n",
      "\n",
      "        [[-1.8863,  0.7562, -1.9874, -0.5637],\n",
      "         [-0.7008,  0.8362, -2.9766, -1.0452],\n",
      "         [-0.3239,  0.1386, -2.0367, -0.8570],\n",
      "         [ 0.1479,  4.3683,  1.4888,  0.0976]],\n",
      "\n",
      "        [[-1.5175,  0.8540,  0.4300,  0.7032],\n",
      "         [-0.2843, -2.2016, -1.7376,  0.7420],\n",
      "         [-1.8132,  1.8642, -2.8904, -1.1910],\n",
      "         [ 1.0095,  1.5861, -0.5034, -0.4327]],\n",
      "\n",
      "        [[-0.4643, -1.2425, -1.0460, -0.2594],\n",
      "         [ 0.3897, -2.1229, -3.0633,  0.7448],\n",
      "         [-0.4158,  0.7986, -0.6721, -0.6962],\n",
      "         [-0.8407,  3.1330,  2.2554, -2.0086]],\n",
      "\n",
      "        [[-1.2746, -0.2482, -0.9748, -1.0539],\n",
      "         [ 0.3897, -2.1229, -3.0633,  0.7448],\n",
      "         [-2.3655,  0.5094, -0.7176, -0.4115],\n",
      "         [-0.4690,  2.6070,  2.2031, -1.3081]],\n",
      "\n",
      "        [[ 0.7987,  0.5588,  0.9235, -0.2636],\n",
      "         [-1.2531, -0.5187, -0.8038, -0.2656],\n",
      "         [-1.5523, -0.6202, -1.3501,  0.6097],\n",
      "         [-1.2100,  2.9526,  0.9941, -0.8222]],\n",
      "\n",
      "        [[ 0.0493,  0.2405, -1.0956,  1.3489],\n",
      "         [-1.3189, -1.3888, -3.6492, -2.5945],\n",
      "         [-1.9816,  0.6656,  0.2850,  0.4518],\n",
      "         [-0.6167,  1.2180,  0.4740,  0.2421]],\n",
      "\n",
      "        [[-1.3491,  2.0526, -2.7454, -0.9397],\n",
      "         [-1.2531, -0.5187, -0.8038, -0.2656],\n",
      "         [-1.8474, -2.1648, -1.6774, -0.6475],\n",
      "         [ 1.1145,  2.7619,  2.9039, -0.8690]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "token_embedding = token_embedding_layer(inputs)\n",
    "pos_embedding = pos_embedding_layer(torch.arange(context_size))\n",
    "\n",
    "input_embedding = token_embedding + pos_embedding\n",
    "\n",
    "print(\"-\" * 100)\n",
    "print(\"token_embedding shape:\", token_embedding.shape)\n",
    "print(\"token_embedding:\\n\", token_embedding)\n",
    "print(\"-\" * 100)\n",
    "print(\"pos_embedding shape:\", pos_embedding.shape)\n",
    "print(\"pos_embedding:\\n\", pos_embedding)\n",
    "print(\"-\" * 100)\n",
    "print(\"inpu_embedding shape:\", input_embedding.shape)\n",
    "print(\"input embedding:\\n\", input_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b45770",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2.2 Sinusoidal Position Embedding\n",
    "\n",
    "In the original transformer's paper, it uses this embedding method.\n",
    "\n",
    "And we first present the math mechanism and implement it in code.\n",
    "\n",
    "Using `sin` and `cos` can indicate the relative position. We know `sin` and `cos` functions are periodic function. For a offset `k` and position `pos+k`'s pos embedding, we can demonstrate it by `pos`'s pos embedding. \n",
    "\n",
    "First we must to know the embedding expression: <br>\n",
    "$$ PE_{(pos, 2i)} = \\sin \\big(\\frac{pos}{1000^{2i / d}} \\big) $$\n",
    "$$ PE_{(pos, 2i + 1)} = \\cos \\big(\\frac{pos}{1000^{2i + 1 / d}} \\big) $$\n",
    "in which $ \\mathbf{i} $ is the dimension index. $ \\mathbf{pos} $ is the position index. $ \\mathbf{d} $ is the model dimension.\n",
    "\n",
    "We have this principle:\n",
    "$$ PE_{pos + k} = \\textbf{M}_k \\cdot PE_{pos} $$\n",
    "\n",
    "Here's proof.\n",
    "\n",
    "Using $\\omega_i = \\frac{1}{1000^{2i / d}}$, so\n",
    "$$ PE_{(pos, 2i)} = \\sin ( \\omega_i * pos ) $$\n",
    "$$ PE_{(pos, 2i + 1)} = \\cos ( \\omega_i * pos) $$\n",
    "Then, consider position `pos + k`'s embedding(having the same dimension index)\n",
    "$$ PE_{(pos + k, 2i)} = \\sin ( \\omega_i * (pos + k)) = \\sin ( \\omega_i * pos + \\omega_i * k) $$\n",
    "$$ PE_{(pos + k, 2i + 1)} = \\cos ( \\omega_i * (pos + k)) = \\cos (\\omega_i * pos + \\omega_i * k) $$\n",
    "let $ A = \\omega_i * pos $, $B = \\omega_i * k $. The original formula is equal to\n",
    "$$ PE_{(pos+k, 2i)}   = \\sin (_i * pos + _i * k) = \\sin (_i * pos) \\cos (_i * k) + \\cos (_i * pos) \\sin (_i * k) $$\n",
    "$$ PE_{(pos+k, 2i+1)} = \\cos (_i * pos + _i * k) = \\cos (_i * pos) \\cos (_i * k) - \\sin (_i * pos) \\sin (_i * k) $$\n",
    "Look closely at the right side of the above two equations. They are exactly a linear combination of $ PE_{(pos, 2i)} $ and $ PE_{(pos, 2i 1)} $:\n",
    "$$ PE_{(pos+k, 2i)}   = [\\cos (_i * k)] * PE_{(pos, 2i)} + [\\sin (_i * k)] * PE_{(pos, 2i+1)} \\tag{1} $$\n",
    "$$ PE_{(pos+k, 2i+1)} = [-\\sin (_i * k)] * PE_{(pos, 2i)} + [\\cos (_i * k)] * PE_{(pos, 2i+1)} \\tag{2} $$\n",
    "\n",
    "And we can use the matrix multiplication to simplify it.\n",
    "let \n",
    "$$ \n",
    "\\bf{M}_k^{(i)} = \\begin{pmatrix}\n",
    "            \\cos (_i * k) & \\sin (_i * k) \\newline\n",
    "            -\\sin (_i * k) & \\cos (_i * k) \\newline\n",
    "            \\end{pmatrix} $$\n",
    "It's a rotation matrix. More importantly, the matrix is only related to paramter: $ k $ the `offset`. It means: The model can be transformed by learning linear transformations $ \\bf{M}_k $ to capture relative position relationships without explicitly training position parameters. If you don't know about matirx, you just need to know the previous version equation `(1)` and `(2)` that don't use matrix and the meaning.\n",
    "\n",
    "> More over, if you want to know about rotation matirx, please click here: [Introduction to Rotaion Matrix](https://articulatedrobotics.xyz/tutorials/coordinate-transforms/rotation-matrices-2d/)\n",
    "\n",
    "Then we implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "070401a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class SinPosEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, d_model: int, max_len: int=5000\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert d_model % 2 == 0, \"d_model must be even\"\n",
    "\n",
    "        # create pos index matrix\n",
    "        position = torch.arange(max_len).unsqueeze(1) # (max_len, 1)\n",
    "\n",
    "        # compute 1 / 1000^{2i / d} here d == d_model\n",
    "        # Here we use the exp and ln to prevent overflow\n",
    "        # 1000^{2i / d_model} = exp(ln(1000^{2i / d_model}))\n",
    "        # = exp(2i / d_model * ln(1000))\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(1000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # compute pos embedding matrix (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # register as untrainable buffer\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: input tensor in other words the original embeded token (batch_size, seq_len, d_model)\n",
    "        return: tensor with pos embedding\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        # check length of sequence\n",
    "        if seq_len > self.max_len:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds max_len {self.max_len}\")\n",
    "        \n",
    "        pos_encoding = self.pe[:seq_len, :]\n",
    "\n",
    "        return x + pos_encoding.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00cab4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100, 512])\n"
     ]
    }
   ],
   "source": [
    "pos_encoder = SinPosEmbedding(d_model=512)\n",
    "\n",
    "x = torch.randn(32, 100, 512)\n",
    "\n",
    "encoded_x = pos_encoder(x)\n",
    "\n",
    "print(encoded_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ba830",
   "metadata": {},
   "source": [
    "-- -\n",
    "### 2.3 RoPE (Rotary Position Embedding)\n",
    "\n",
    "It's now most the LLMs' embedding method.\n",
    "\n",
    "Here we also present its math machanism and implement it in code.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    <h4>\n",
    "    Note\n",
    "    </h4>\n",
    "    <p>\n",
    "        Below is all prerequisite knowledge and math proof, if you are not interested in it, you can skip it. However in my advice, you can simply read it, cause I write it very very simple for you. No intermediate steps are omitted. So you can just read it. About this part of math mechanism, you can also read it here <a href=\"https://arxiv.org/pdf/2104.09864\">Original Paper of RoPE</a>. The following proof can be seen as presengting some Omitted intermediate steps.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "<hr style=\"border: 0; height: 3px; background: linear-gradient(90deg, #ff0000, #00ff00); margin: 1em 0;\">\n",
    "\n",
    "0. Background:\n",
    "    <br>\n",
    "\n",
    "    RoPE is relative to a module named transformer. That we will introduce after this in [transformer](../transformer/transformer.ipynb). And here we simply talk about it. In transformer, it recieves the text, actually after the embedding i.e. word vector with the pos embedding. It will use three matrix: $ Q $, $ K $, $ V $ multiply every input text generating three new vector named: $ q $, $ k $, $ v $. Then compute the dot mutiplication of $ q $ and $ k $. Remeber that we have a the pos embedding on each word of the text. So the $ q $, $ k $ of the word also have it. Then we could begin derivation.\n",
    "\n",
    "Before that, we should have some more math base. Below we will introduce.\n",
    "\n",
    "1. Complex number:\n",
    "    <br>\n",
    "\n",
    "    Note: $ i = \\sqrt{-1} $, $ i^{2} = -1 $.\n",
    "    And the complex number's base format is: $ a + b i$, in which $ a $ and $ b $ all are real number. In which we call $ a $ the real part. $ b $ the imaginative part.\n",
    "    We also have another notation of complex number. Do you know the god's formula? It like this: $ e^{i \\pi} + 1 = 0 $. It comes from \n",
    "    $$ \n",
    "    e^{i \\theta} = \\cos \\theta + i \\sin \\theta \\tag{3} \n",
    "    $$ \n",
    "    which is called complex number's **complex exponential form** or called **Euler's formula**. If you let $ \\theta $ equals to $ \\pi $.\n",
    "    $$\n",
    "    e^{i \\pi} = \\cos \\pi + i \\sin \\pi\n",
    "    $$\n",
    "    We know $ \\cos \\pi = -1 $, $ \\sin \\pi = 0 $, so we get $ e^{i \\pi} + 1 = 0 $.\n",
    "    Here, we take a example or deep into it. Like real number, we have Cartesian coordinate system. With it we can decide a point by its `x` axis coordinate and `y` axis coordinate. Also we can use this point to (0, 0) distance $ r $, and $ \\theta $ the angle with the x-axis to decide its position. That's the same in complex number. But the Cartesian coordinate system of complex number we call it **Complex plane**. Its x axis is the same with Cartesian coordinate system. However the y axis's coordinate unit is $ i $. So the complex number $ 3 + 4 i $ in the complex plane like this:\n",
    "\n",
    "    <center><img src=https://github.com/gzqccnu/img/blob/main/complex_plane.png?raw=true)></center>\n",
    "\n",
    "    In which, $ r = \\sqrt{3^{2} + 4^{2}} = 5 $. Generally, given a complex number $ a + b i $, its $ r $ equals to $ \\sqrt{a^{2} + b^{2}} $. \n",
    "\n",
    "2. Conjugate complex number:\n",
    "    <br>\n",
    "\n",
    "    $ a + b i $'s conjugate complex number equals to $ a - b i $. That's to say: one complex number and its conjugate complex number they have the same real part but the opposite imaginary part. If we note $ x = a + b i $, then we have $ x^{*} = a - b i $.  We expand to **complex exponential form**. If we have a complex number $ a + bi $, \n",
    "    we can convert it to(assume $ \\theta $ stands for its angle in complex plane ): $ \\cos \\theta + i \\sin \\theta $. And its conjugate complex number can expressed in: $ \\cos \\theta - i \\sin \\theta $.\n",
    "3. Dot of complex numbers:\n",
    "    <br>\n",
    "\n",
    "    Consider we have two complex numbers: $ x = a + b i $ and $ y = c + d i $. Then we want to compute the dot mutiplication of $ x $ and $ y $.\n",
    "    We have the principle:\n",
    "    $$\n",
    "    x \\cdot y = \\langle x, y* \\rangle = (a + b i) \\cdot (c - d i) = ac + b(-d)i^{2} = ac + bd \\tag{4}\n",
    "    $$\n",
    "\n",
    "Note we begin with a $ \\textbf{2D} $ case.\n",
    "Consider we already have the $ \\boldsymbol{x}_{q} $ and $ \\boldsymbol{x}_{k} $. Their positions are respectly $ m $ and $ n $. Now we must to find a way or function:\n",
    "$$\n",
    "\\boldsymbol{f} (\\boldsymbol{vec}, pos)\n",
    "$$\n",
    "So we have \n",
    "$$\n",
    "\\boldsymbol{q}_{m} = \\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) \\tag{5}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{k}_{n} = \\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}. n) \\tag{6}\n",
    "$$\n",
    "\n",
    "for vector $ \\boldsymbol{q}_{m} \\cdot \\boldsymbol{k}_{n} $ only depend on relative position $ m-n $. And now consider this case: we have vectors $ \\boldsymbol{A} $ and vector $ \\boldsymbol{B} $ are parallel to x-axis. Then we rotate $ \\boldsymbol{A} $ by $ +30 $ degrees with $ \\boldsymbol{B} $ by $ +40 $ degrees. And the angle between them is 10 degrees. Expressed in math is:\n",
    "$$\n",
    "\\cos \\langle \\boldsymbol{A}, \\boldsymbol{B} \\rangle = \\frac{\\boldsymbol{A} \\cdot \\boldsymbol{B}}{||A|| \\ ||B||}\n",
    "$$\n",
    "Analogy to position encoding, we see position $ m $ as a rotation angle $ m \\theta $. So, the dot multiplication of $ \\boldsymbol{q}_{m} $ and $ \\boldsymbol{k}_{n} $ only depends on $ (m - n) \\theta $ i.e. the relative position $ m - n $.\n",
    "\n",
    "That's it. Now we formally entry proof.\n",
    "\n",
    "Note the function only depends on relative position, that's to say, it has the property:\n",
    "$$\n",
    "\\boldsymbol{q}_{m}^{\\boldsymbol{T}} \\boldsymbol{k}_{n} = \\boldsymbol{f}(\\boldsymbol{x}_{q}, m) \\cdot \\boldsymbol{f}(\\boldsymbol{x}_{k}, n) = \\langle \\boldsymbol{f}(\\boldsymbol{x}_{q}, m), \\boldsymbol{f}(\\boldsymbol{x}_{k}, n) \\rangle = g(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, m - n) \\tag{7}\n",
    "$$\n",
    "$ g $ is for better reading experience.\n",
    "\n",
    "To solve the identity equation, we must to have some initial conditions:\n",
    "$$\n",
    "\\boldsymbol{f}_{q} (\\boldsymbol{x}_{q}, 0) = \\boldsymbol{q} \\tag{8}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k} (\\boldsymbol{x}_{k}, 0) = \\boldsymbol{k} \\tag{9}\n",
    "$$\n",
    "\n",
    "Based on the **complex exponential form** and take advantage of the geometric meaning of vector in $ \\textbf{2D} $ and its complex counter part,\n",
    "decompose functions in Equations (5) and (6) into\n",
    "$$\n",
    "\\boldsymbol{f}_{q} (\\boldsymbol{q}, m) = R_{q}(\\boldsymbol{x}_{q}, m) e^{i \\Theta_{q}(\\boldsymbol{x}_{q}, m)} \\tag{9}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = R_{k}(\\boldsymbol{x}_{k}, n) e^{i \\Theta_{k}(\\boldsymbol{x}_{k}, n)} \\tag{10}\n",
    "$$\n",
    "(6) into\n",
    "$$\n",
    "g(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m) = R_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m) e^{i \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m)}\n",
    "$$\n",
    "\n",
    "where $ R_{f} $ , $ R_{g} $ and $ \\Theta_{f} $ , $ \\Theta_{g} $ are the radical and angular components for $ \\boldsymbol{f}_{\\{q, k\\}} $ and $ g $, respectively. Plug them into Equation (7), we get the relation:\n",
    "$$\n",
    "R_{q}(\\boldsymbol{x}_{q}, m)R_{k}(\\boldsymbol{x}_{k}, n) = R_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m) \\tag{11}\n",
    "$$\n",
    "$$\n",
    "\\Theta_{k}(\\boldsymbol{x}_{k}, n) - \\Theta_{q}(\\boldsymbol{x}_{q}, m) = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m) \\tag{12}\n",
    "$$\n",
    "\n",
    "<hr style=\"border: 3px dashed #ccc; width: 100%;\">\n",
    "\n",
    "Here, I want to explain why in \n",
    "$$\n",
    "\\Theta_{k}(\\boldsymbol{x}_{k}, n) - \\Theta_{q}(\\boldsymbol{x}_{q}, m) \n",
    "$$\n",
    "genrate `negative sign`. Do you remember Equation (4). We say two complex number's dot product is equal to the product of the conjugate of one complex number and another complex number. Using **complex exponential form**, we have\n",
    "$$\n",
    "x = a + bi = \\sqrt{a^{2} + b^{2}} (\\cos \\theta + i \\sin \\theta) = \\sqrt{a^{2} + b^{2}} e^{i \\theta}\n",
    "$$\n",
    "$$\n",
    "y = c + di = \\sqrt{c^{2} + d^{2}} (\\cos \\theta + i \\sin \\theta) = \\sqrt{c^{2} + d^{2}} e^{i \\theta}\n",
    "$$\n",
    "So we have\n",
    "$$\n",
    "y^{*} = c - di = \\sqrt{c^{2} + d^{2}} (\\cos \\theta - i \\sin \\theta) \\tag{13}\n",
    "$$\n",
    "And how its complex exponential form like? Do you remember formula (3)?\n",
    "We could Solving by substitution method. \n",
    "We know for a complex number: $ a + b i$, its complex exponential form is: \n",
    "$$ \n",
    "r e^{i \\theta} = r ( \\cos \\theta + i \\sin \\theta ) \n",
    "$$ \n",
    "It's conjugate form like:  \n",
    "$$ \n",
    "r ( \\cos \\theta - i \\sin \\theta ) \n",
    "$$\n",
    "If we use $ i(-\\theta) $ replace $ \\theta $ in that form, we get\n",
    "$$\n",
    "e^{i (-\\theta)} = \\cos (-\\theta) + i \\sin (-\\theta) \\tag{14}\n",
    "$$\n",
    "We know\n",
    "$$\n",
    "\\cos (-\\theta) = \\cos \\theta\n",
    "$$\n",
    "$$\n",
    "\\sin (-\\theta) = - \\sin \\theta\n",
    "$$\n",
    "So equation (12) like\n",
    "$$\n",
    "e^{i (-\\theta)} = \\cos \\theta - i \\sin \\theta\n",
    "$$\n",
    "So equation (13) equals to\n",
    "$$\n",
    "y^{*} = c - di = \\sqrt{c^{2} + d^{2}} (\\cos \\theta - i \\sin \\theta) = \\sqrt{c^{2} + d^{2}} e^{ - i \\theta}\n",
    "$$\n",
    "According to the law of exponentiation, we obtain equation (11) and (12)\n",
    "<hr style=\"border: 3px dashed #ccc; width: 100%;\">\n",
    "with the corresponding initial condition as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{q} = ||\\boldsymbol{q}|| e^{i \\theta_{q}} = R_{q}(\\boldsymbol{x}_{q}, 0) e^{i \\Theta_{q}(\\boldsymbol{x}_{q}, 0)}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{k} = ||\\boldsymbol{k}|| e^{i \\theta_{k}} = R_{k}(\\boldsymbol{x}_{k}, 0) e^{i \\Theta_{k}(\\boldsymbol{x}_{k}, 0)}\n",
    "$$\n",
    "where $ ||\\boldsymbol{q}|| $, $ ||\\boldsymbol{k}|| $ and $ \\theta_{q} $, $ \\theta_{k} $ are the radial and angular part of $ \\boldsymbol{q} $ and $ \\boldsymbol{k} $ on the $ \\textbf{2D} $ plane.\n",
    "Next, we set $ m = n $ in Equation (11) (12) and take into account initial conditions in Equation (8) (9):\n",
    "$$\n",
    "R_{q}(\\boldsymbol{x}_{q}, m)R_{k}(\\boldsymbol{x}_{k}, m) = R_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0) = R_{q}(\\boldsymbol{x}_{q}, 0)R_{k}(\\boldsymbol{x}_{k}, 0) = ||\\boldsymbol{q}|| \\ ||\\boldsymbol{k}|| \\tag{15}\n",
    "$$\n",
    "$$\n",
    "\\Theta_{k}(\\boldsymbol{x}_{k}, m) - \\Theta_{q}(\\boldsymbol{x}_{q}, m) = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0) = \\Theta_{k}(\\boldsymbol{x}_{k}, 0) - \\Theta_{q}(\\boldsymbol{x}_{q}, 0) = \\theta_{k} - \\theta_{q} \\tag{16}\n",
    "$$\n",
    "for we have\n",
    "$$\n",
    "R_{g}(\\boldsymbol{x}_{q}, m) = R_{q}(\\boldsymbol{x}_{q}, 0) = ||\\boldsymbol{q}||\n",
    "$$\n",
    "$$\n",
    "R_{k}(\\boldsymbol{x}_{k}, n) = R_{k}(\\boldsymbol{x}_{k}, 0) = ||\\boldsymbol{k}||\n",
    "$$\n",
    "$$\n",
    "R_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, n - m) = R_{g} (\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 0) = ||\\boldsymbol{q}|| \\ ||\\boldsymbol{k}||\n",
    "$$\n",
    "interprets that the radial functions $ R_{q} $ ,$ R_{k} $ and $ R_{g} $ are independent from the position information.\n",
    "\n",
    "For Equation (16), we do a transposition then get:\n",
    "$$\n",
    "\\Theta_{q}(\\boldsymbol{x}_{}, m) - \\theta_{q} = \\Theta_{k}(\\boldsymbol{x}_{k}, n) - \\theta_{k}\n",
    "$$\n",
    "indicates that the angular functions does not dependent on $ \\boldsymbol{x} $\n",
    "so the term \n",
    "$$\n",
    "\\Theta_{f}(\\boldsymbol{x}_{\\{q, k\\}}, m ) - \\theta_{\\{q, k\\}}\n",
    "$$\n",
    "is a function of $ m $ and we denote it as $ \\phi(m) $, yielding:\n",
    "$$\n",
    "\\Theta_{f}(\\boldsymbol{x}_{\\{q, k\\}}, m ) = \\phi(m) + \\theta_{\\{q, k\\}} \\tag{17}\n",
    "$$\n",
    "Further, by plugging $ n = m + 1$ to Equation (11) (12) we get\n",
    "$$\n",
    "\\Theta_{k}(\\boldsymbol{x}_{k}, m + 1) - \\Theta_{q}(\\boldsymbol{x}_{q}, m) = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 1)\n",
    "$$\n",
    "and consider the above Equation (17) we have\n",
    "$$\n",
    "(\\phi(m + 1) + \\theta_{k}) - (\\phi(m) + \\theta_{q}) = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 1)\n",
    "$$\n",
    "so we get\n",
    "$$\n",
    "\\phi(m + 1) - \\phi(m) = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 1) + \\theta_{q} - \\theta_{k}\n",
    "$$\n",
    "Since RHS is a constant irrelevanttom $ \\phi(m) $ with continuous integer inputs produce an arithmetic progression, we set \n",
    "$$\n",
    "\\theta = \\Theta_{g}(\\boldsymbol{x}_{q}, \\boldsymbol{x}_{k}, 1) + \\theta_{q} - \\theta_{k}\n",
    "$$\n",
    "then set $ m $ to $(1, 2, 3..) $, we get:\n",
    "$$\n",
    "\\phi(1) - \\phi(0) = \\theta\n",
    "$$\n",
    "$$\n",
    "\\phi(2) - \\phi(1) = \\theta\n",
    "$$\n",
    "$$\n",
    "\\phi(3) - \\phi(2) = \\theta\n",
    "$$\n",
    "$$\n",
    "\\vdots\n",
    "$$\n",
    "$$\n",
    "\\phi(m) - \\phi(m - 1) = \\theta\n",
    "$$\n",
    "So we get:\n",
    "$$\n",
    "\\phi(m) - \\phi(0) = m \\theta\n",
    "$$\n",
    "we set\n",
    "$$\n",
    "\\phi(0) = \\gamma\n",
    "$$\n",
    "we plus all above differential terms regarding $ \\phi $\n",
    "$$\n",
    "\\phi(m) - \\phi(m - 1) = \\theta \\newline\n",
    "+ \\newline\n",
    "\\phi(m - 1) - \\phi(m - 2) = \\theta \\newline\n",
    "+ \\newline\n",
    "\\vdots \\newline\n",
    "+ \\newline\n",
    "\\phi(3) - \\phi(2) = \\theta \\newline\n",
    "+ \\newline\n",
    "\\phi(2) - \\phi(1) = \\theta \\newline\n",
    "+ \\newline\n",
    "\\phi(1) - \\phi(0) = \\theta\n",
    "$$ \n",
    "finally we get\n",
    "$$\n",
    "\\phi(m) = m \\theta + \\gamma \\tag{18}\n",
    "$$\n",
    "where $ \\theta $, $ \\gamma \\in \\mathbb{R}$ are constants and $ \\theta $ is non-zero.\n",
    "\n",
    "Equation (18) reveal:\n",
    "$$\n",
    "\\phi(m) = \\Theta_{f}(\\boldsymbol{x}_{\\{q, k\\}}, m ) - \\theta_{\\{q, k\\}} = m \\theta + \\gamma\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\Theta_{f}(\\boldsymbol{x}_{\\{q, k\\}}, m ) = \\theta_{\\{q, k\\}} + m \\theta + \\gamma\n",
    "$$\n",
    "so we have equation (9) and (10)\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = R_{q}(\\boldsymbol{x}_{q}, m) e^{i \\Theta_{q}(\\boldsymbol{x}_{q}, m)} = R_{q}(\\boldsymbol{x}_{q}, m) e^{i (\\theta_{q} + m \\theta + \\gamma)} = ||\\boldsymbol{q}|| e^{i (\\theta_{q} + m \\theta + \\gamma)} \\tag{19}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = R_{k}(\\boldsymbol{x}_{k}, n) e^{i \\Theta_{k}(\\boldsymbol{x}_{k}, n)} = R_{k}(\\boldsymbol{x}_{k}, n) e^{i (\\theta_{k} + n \\theta + \\gamma)} = ||\\boldsymbol{k}|| ^{i (\\theta_{k} + n \\theta + \\gamma)} \\tag{20}\n",
    "$$\n",
    "in view of\n",
    "$$\n",
    "||\\boldsymbol{q}|| e^{i \\theta_{q}} = \\boldsymbol{q}\n",
    "$$\n",
    "$$\n",
    "||\\boldsymbol{k}||e^{i \\theta_{k}} = \\boldsymbol{k}\n",
    "$$\n",
    "we plug them into Equation (19) and (20), we get:\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = \\boldsymbol{q} e^{i(m \\theta + \\gamma)}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = \\boldsymbol{k} e^{i (n \\theta + \\gamma)}\n",
    "$$\n",
    "we set $ \\gamma = 0 $,\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = \\boldsymbol{q} e^{i m \\theta} \\tag{21}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = \\boldsymbol{k} e^{i n \\theta} \\tag{22}\n",
    "$$\n",
    "we define \n",
    "$$\n",
    "\\boldsymbol{q} = \\boldsymbol{f}(\\boldsymbol{x}_{m}, 0) = \\boldsymbol{W}_{q} \\boldsymbol{x}_{m}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{k} = \\boldsymbol{f}(\\boldsymbol{x}_{n}, 0) =\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}\n",
    "$$\n",
    "where $ W_{q} $, $ W_{k} $ are all 2D matrix. Plug them into Equation (21) (22) we get:\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = (\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}) e^{i m \\theta} \\tag{23}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = (\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}) e^{i n \\theta} \\tag{24}\n",
    "$$\n",
    "So finally we get the position embedding function in $ \\textbf{2D} $\n",
    "<hr style=\"border: 3px dashed #ccc; width: 100%;\">\n",
    "\n",
    "Here we will introduce you a new theory a complex number can be expressed in **matrix** like:\n",
    "$$\n",
    "a + bi = \\sqrt{a^{2} + b^{2}} e^{i \\theta} = \\sqrt{a^{2} + b^{2}} \\begin{pmatrix}\n",
    "             \\cos \\theta & - \\sin \\theta \\newline\n",
    "             \\sin \\theta & \\cos \\theta \\newline\n",
    "             \\end{pmatrix} = \\begin{pmatrix}\n",
    "                             a & -b \\newline\n",
    "                             b & a \\newline\n",
    "                             \\end{pmatrix} \\tag{25}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\theta = \\arctan (\\frac{b}{a})\n",
    "$$\n",
    "Here we do not present strict mathematical derivation proofs, rather conduct validating proofs from algebraic operations perspective.\n",
    "\n",
    "Assume we have two complex number:\n",
    "$$\n",
    "z_{1} = a + bi \\tag{26}\n",
    "$$\n",
    "$$\n",
    "z_{2} = c + di \\tag{27}\n",
    "$$\n",
    "according to Equation (25) we get the equivalent form of Equation (26)\n",
    "$$\n",
    "z_{1} = \\begin{pmatrix}\n",
    "        a & - b \\newline\n",
    "        b & a \\newline\n",
    "        \\end{pmatrix} \\tag{28}\n",
    "$$\n",
    "and Equation (27)\n",
    "$$\n",
    "z_{2} = \\begin{pmatrix}\n",
    "        c & -d \\newline\n",
    "        d & c \\newline\n",
    "        \\end{pmatrix} \\tag{29}\n",
    "$$\n",
    "their product(not dot product) in Equation (26) (27) form, we get\n",
    "$$\n",
    "z_{1} z_{2} = (a + bi)(c + di) = ac + ad i + bc i + bd(i^{2}) = (ac - bd) + (ad + bc)i \\tag{30}\n",
    "$$\n",
    "in Equation (28) (29) form, we get\n",
    "$$\n",
    "z_{1} z_{2} = \n",
    "        \\begin{pmatrix}\n",
    "        a & - b \\newline\n",
    "        b & a \\newline\n",
    "        \\end{pmatrix} \n",
    "        \\begin{pmatrix}\n",
    "        c & -d \\newline\n",
    "        d & c \\newline\n",
    "        \\end{pmatrix} = \n",
    "        \\begin{pmatrix}\n",
    "        ac - bd & - (ad + bc) \\newline\n",
    "        ad + bc & ac - bd \\newline\n",
    "        \\end{pmatrix}\n",
    "$$\n",
    "and return it to complex number form, it equals to Equation (30)\n",
    "<hr style=\"border: 3px dashed #ccc; width: 100%;\">\n",
    "\n",
    "According to Equation (25), we have new version of Equation (23) (24)\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = (\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}) \\begin{pmatrix}\n",
    "                                                                                    \\cos m \\theta & - \\sin m \\theta \\newline\n",
    "                                                                                    \\sin m \\theta & \\cos m \\theta\n",
    "                                                                                    \\end{pmatrix}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = (\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}) \\begin{pmatrix}\n",
    "                                                                                    \\cos n \\theta & - \\sin n \\theta \\newline\n",
    "                                                                                    \\sin n \\theta & \\cos n \\theta\n",
    "                                                                                    \\end{pmatrix}\n",
    "$$\n",
    "We can move $ e^{\\{m, n\\} \\theta} $ forward, and the rotation matrix can also be moved forward.\n",
    "$$\n",
    "\\boldsymbol{f}_{q}(\\boldsymbol{x}_{q}, m) = \\begin{pmatrix}\n",
    "                                            \\cos m \\theta & - \\sin m \\theta \\newline\n",
    "                                            \\sin m \\theta & \\cos m \\theta\n",
    "                                            \\end{pmatrix} (\\boldsymbol{W}_{q} \\boldsymbol{x}_{m}) \\tag{31}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{f}_{k}(\\boldsymbol{x}_{k}, n) = \\begin{pmatrix}\n",
    "                                            \\cos n \\theta & - \\sin n \\theta \\newline\n",
    "                                            \\sin n \\theta & \\cos n \\theta\n",
    "                                            \\end{pmatrix} (\\boldsymbol{W}_{k} \\boldsymbol{x}_{n}) \\tag{32}\n",
    "$$\n",
    "cause all the above are in 2D dimension. So we can write Equation (31) (32) in this unified format\n",
    "$$\n",
    "\\boldsymbol{f}_{\\{q, k\\}}(\\boldsymbol{x}_{m}, m) = \\begin{pmatrix}\n",
    "                                                          \\cos m \\theta & - \\sin m \\theta \\newline\n",
    "                                                          \\sin m \\theta  & \\cos m \\theta\n",
    "                                                          \\end{pmatrix}\n",
    "                                                          \\begin{pmatrix}\n",
    "                                                          W_{\\{q, k\\}}^{11} & W_{\\{q, k\\}}^ {12} \\newline\n",
    "                                                          W_{\\{q, k\\}}^{21} & W_{\\{q, k\\}}^{22}\n",
    "                                                          \\end{pmatrix}\n",
    "                                                          \\begin{pmatrix}\n",
    "                                                          x_{m}^{(1)} \\newline\n",
    "                                                          x_{m}^{2}\n",
    "                                                          \\end{pmatrix} \\tag{33}\n",
    "$$\n",
    "where $ (x_{m}^{(1)}, x_{m}^{(2)}) $ is $ x_{m} $ expressed in the 2D coordinate.\n",
    "Then we promoted Equation (33) to a general form\n",
    "$$\n",
    "f_{\\{q,k\\}}(\\boldsymbol{x}_m, m) = R_{\\Theta, m}^d \\, W_{\\{q,k\\}} \\, \\boldsymbol{x}_m\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "R_{\\Theta, m}^d = \\begin{pmatrix}\n",
    "\\cos m\\theta_1 & -\\sin m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\newline\n",
    "\\sin m\\theta_1 & \\cos m\\theta_1 & 0 & 0 & \\cdots & 0 & 0 \\newline\n",
    "0 & 0 & \\cos m\\theta_2 & -\\sin m\\theta_2 & \\cdots & 0 & 0 \\newline\n",
    "0 & 0 & \\sin m\\theta_2 & \\cos m\\theta_2 & \\cdots & 0 & 0 \\newline\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\newline\n",
    "0 & 0 & 0 & 0 & \\cdots & \\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\newline\n",
    "0 & 0 & 0 & 0 & \\cdots & \\sin m\\theta_{d/2} & \\cos m\\theta_{d/2}\n",
    "\\end{pmatrix} \\tag{34}\n",
    "$$\n",
    "$$\n",
    "x = \\begin{bmatrix}\n",
    "    x_{1} \\newline\n",
    "    x_{2} \\newline\n",
    "    x_{3} \\newline\n",
    "    \\vdots \\newline\n",
    "    x_{d} \\newline\n",
    "    \\end{bmatrix} \\tag{35}\n",
    "$$\n",
    "and we split matrix (34) to block matrix format\n",
    "$$\n",
    "R_{\\Theta, m}^d = \\begin{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "\\cos m\\theta_1 & - \\sin m\\theta_1 \\newline\n",
    "\\sin m\\theta_1 & \\cos m\\theta_1 \\newline\n",
    "\\end{pmatrix}\n",
    "& \\begin{pmatrix} 0 & 0 \\newline 0 & 0 \\newline \\end{pmatrix}\n",
    "& \\cdots\n",
    "& \\begin{pmatrix} 0 & 0 \\newline 0 & 0 \\newline \\end{pmatrix} \\newline[6pt]\n",
    "\n",
    "\\begin{pmatrix} 0 & 0 \\newline 0 & 0  \\newline \\end{pmatrix}\n",
    "& \\begin{pmatrix}\n",
    "\\cos m\\theta_2 & -\\sin m\\theta_2 \\newline\n",
    "\\sin m\\theta_2 & \\cos m\\theta_2 \\newline\n",
    "\\end{pmatrix}\n",
    "& \\cdots\n",
    "& \\begin{pmatrix} 0 & 0 \\newline 0 & 0 \\end{pmatrix} \\newline[6pt]\n",
    "\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\newline[6pt]\n",
    "\n",
    "\\begin{pmatrix} 0 & 0 \\newline 0 & 0  \\newline \\end{pmatrix}\n",
    "& \\begin{pmatrix} 0 & 0 \\newline 0 & 0  \\newline \\end{pmatrix}\n",
    "& \\cdots\n",
    "& \\begin{pmatrix}\n",
    "\\cos m\\theta_{d/2} & -\\sin m\\theta_{d/2} \\newline\n",
    "\\sin m\\theta_{d/2} & \\cos m\\theta_{d/2} \\newline\n",
    "\\end{pmatrix}\n",
    "\\end{pmatrix} \\tag{36}\n",
    "$$\n",
    "with vector (35)\n",
    "$$\n",
    "\\boldsymbol{x} = \\begin{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    x_{1} \\newline\n",
    "    x'_{1} \\newline \n",
    "    \\end{bmatrix} \\newline\n",
    "    \\begin{bmatrix}\n",
    "    x_{2} \\newline\n",
    "    x'_{2} \\newline \n",
    "    \\end{bmatrix} \\newline\n",
    "    \\vdots \\newline\n",
    "    \\begin{bmatrix}\n",
    "    x_{d / 2} \\newline\n",
    "    x'_{d / 2} \\newline \n",
    "    \\end{bmatrix}\n",
    "    \\end{bmatrix} \\tag{37}\n",
    "$$\n",
    "we take a submatrix from matrix (36) except all zero matrix\n",
    "$$\n",
    "\\boldsymbol{M}_{i} = \\begin{pmatrix}\n",
    "                     \\cos m \\theta_{i} & - \\sin m \\theta_{i} \\newline\n",
    "                     \\sin m \\theta_{i} & \\sin m \\theta_{i} \\newline\n",
    "                     \\end{pmatrix} \\ \\text{or}\n",
    "                     \\begin{pmatrix}\n",
    "                     0 & 0 \\newline\n",
    "                     0 & 0 \\newline\n",
    "                     \\end{pmatrix}\n",
    "$$\n",
    "we see $ \\boldsymbol{M}_{i} $ as a basic element of matrix (36)\n",
    "\n",
    "we take a subvector from vector (37)\n",
    "$$\n",
    "\\boldsymbol{x}_{i} = \\begin{bmatrix}\n",
    "                     x_{i} \\newline\n",
    "                     x'_{i} \\newline\n",
    "                     \\end{bmatrix}\n",
    "$$\n",
    "denote it to $ \\boldsymbol{x}_{i} $\n",
    "\n",
    "we see $ \\boldsymbol{x}_{i} $ as a basic element of vector $ \\boldsymbol{x} $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1df4170",
   "metadata": {},
   "source": [
    "<a id=\"matrix-vector\"></a>\n",
    "\n",
    "<span style=\"color: red;\">\n",
    "    When matrix (36) multiply vector (37), the computing algorithm of them is: \n",
    "    Multiply each element in each row of the matrix by each element in x one by one. \n",
    "    That's to say we see a row of the matrix as a vector, and what we do is let the row vector \n",
    "    of the matrix multiply the vector x and the result is still the row vector of the matrix. \n",
    "    we can see it as below\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b90be3f",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{row\\_vec}_{i} = \\begin{bmatrix}\n",
    "                 \\boldsymbol{M}_{i, 1} & \\boldsymbol{M}_{i, 2} & \\cdots & \\boldsymbol{M}_{i, d / 2}\n",
    "                 \\end{bmatrix}\n",
    "$$\n",
    "where $ i $ is row index of matrix, $ i \\in \\{1, 2, \\cdots, d / 2\\}$. \n",
    "$$\n",
    "\\boldsymbol{x} = \\begin{bmatrix}\n",
    "                 \\boldsymbol{x}_{1} & \\boldsymbol{x}_{2} & \\cdots & \\boldsymbol{x}_{d / 2}\n",
    "                 \\end{bmatrix}\n",
    "$$\n",
    "so the new row vector is\n",
    "$$\n",
    "\\boldsymbol{new\\_row\\_vec}_{i} = \\begin{bmatrix}\n",
    "                           \\boldsymbol{M}_{i, 1} * \\boldsymbol{x}_{1} & \\boldsymbol{M}_{i, 2} * \\boldsymbol{x}_{2} & \\cdots & \\boldsymbol{M}_{i, d / 2} \\boldsymbol{x}_{d / 2}\n",
    "                           \\end{bmatrix}\n",
    "$$\n",
    "so when matrix (36) multiply vector (37), the final result like\n",
    "$$\n",
    "R_{\\Theta, m}^d \\boldsymbol{x} = \\begin{pmatrix}\n",
    "\n",
    "                                    \\begin{pmatrix}\n",
    "                                    \\cos m \\theta_{1} & - \\sin m \\theta_{1} \\newline\n",
    "                                    \\sin m \\theta_{1} & \\cos m \\theta_{1} \\newline\n",
    "                                    \\end{pmatrix}\n",
    "                                    \\begin{bmatrix}\n",
    "                                    x_{1} \\newline\n",
    "                                    x'_{1} \\newline\n",
    "                                    \\end{bmatrix} & \\begin{pmatrix}\n",
    "                                                    0 \\newline\n",
    "                                                    0 \\newline\n",
    "                                                    \\end{pmatrix} & \\cdots & \\begin{pmatrix}\n",
    "                                                                             0 \\newline\n",
    "                                                                             0 \\newline\n",
    "                                                                             \\end{pmatrix} \n",
    "\n",
    "                                    \\newline\n",
    "\n",
    "                                    \\begin{pmatrix}\n",
    "                                    0 \\newline\n",
    "                                    0 \\newline\n",
    "                                    \\end{pmatrix} & \\begin{pmatrix}\n",
    "                                                    \\cos m \\theta_{2} & - \\sin m \\theta_{2} \\newline\n",
    "                                                    \\sin m \\theta_{2} & \\cos m \\theta_{2} \\newline\n",
    "                                                    \\end{pmatrix}\n",
    "                                                    \\begin{bmatrix}\n",
    "                                                    x_{2} \\newline\n",
    "                                                    x'_{2} \\newline\n",
    "                                                    \\end{bmatrix} & \\cdots & \\begin{pmatrix}\n",
    "                                                                            0 \\newline\n",
    "                                                                            0 \\newline\n",
    "                                                                             \\end{pmatrix}\n",
    "\n",
    "                                    \\newline\n",
    "\n",
    "                                    \\vdots & \\vdots & \\ddots & \\vdots\n",
    "\n",
    "                                    \\newline\n",
    "\n",
    "                                    \\begin{pmatrix}\n",
    "                                    0 \\newline\n",
    "                                    0 \\newline\n",
    "                                    \\end{pmatrix} & \\begin{pmatrix}\n",
    "                                                    0 \\newline\n",
    "                                                    0 \\newline\n",
    "                                                    \\end{pmatrix} & \\cdots & \\begin{pmatrix}\n",
    "                                                                             \\cos m \\theta_{d / 2} & - \\sin m \\theta_{d / 2} \\newline\n",
    "                                                                             \\sin m \\theta_{d / 2} & \\cos m \\theta_{d / 2} \\newline\n",
    "                                                                             \\end{pmatrix}\n",
    "                                                                             \\begin{bmatrix}\n",
    "                                                                             x_{d / 2} \\newline\n",
    "                                                                             x'_{d / 2} \\newline\n",
    "                                                                             \\end{bmatrix}\n",
    "\n",
    "                                 \\end{pmatrix} \\tag{38}\n",
    "$$\n",
    "and we can see many zero-submatrix in it, and when computing we actually don't need to compute them, we just to compute the **diagonal part**, that will speed computation up. And we can also not storage the hole matrix with lots of zero-submatrix. That will free a lot of memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8838a530",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0; height: 3px; background: linear-gradient(90deg, #ff0000, #00ff00); margin: 1em 0;\">\n",
    "\n",
    "Above is the ending of math proof. Here we give a more  computional efficient realization of multiplication of $ R_{\\Theta}^{d} $ and $ \\boldsymbol{x} \\in \\mathbb{R}^{d} $. From Equation (38) we know We just need to compute\n",
    "$$\n",
    "e = \\boldsymbol{M}_{i} \\ \\boldsymbol{x}_{i} = \\begin{pmatrix}\n",
    "                                            \\cos m \\theta_{i} & - \\sin m \\theta_{i} \\newline\n",
    "                                            \\sin m \\theta_{i} & \\cos m \\theta_{i}\n",
    "                                            \\end{pmatrix}\n",
    "                                            \\begin{bmatrix}\n",
    "                                            x_{i} \\newline\n",
    "                                            x'_{i}\n",
    "                                            \\end{bmatrix} \\tag{39}\n",
    "$$\n",
    "on the **diagonal part**.\n",
    "\n",
    "Notice we split vector $ \\boldsymbol{x} $ to two elements in a group like Equation (37). So when implementing in code, we should do first. Then we expand the computation formula (39). We said above, when multiplying a matrix with a vector, we see the row of the matrix as a vector. Then we multiply the row vector with given vector.\n",
    "$$\n",
    "\\boldsymbol{fir\\_row} = \\begin{bmatrix}\n",
    "                 \\cos m \\theta_{i} & - \\sin m \\theta_{i}\n",
    "                 \\end{bmatrix} \\newline\n",
    "\\boldsymbol{sec\\_row} = \\begin{bmatrix}\n",
    "                 \\sin m \\theta_{i} & \\cos m \\theta_{i}\n",
    "                 \\end{bmatrix}\n",
    "$$\n",
    "then compute as\n",
    "$$\n",
    "\\boldsymbol{fir\\_row} \\ \\boldsymbol{x}_{i} = \\cos m \\theta_{i} \\ x_{i} - \\sin m \\theta_{i} \\ x'_{i} = \\begin{pmatrix}\n",
    "                                                                                                       x_{i} \\newline\n",
    "                                                                                                       - x'_{i}\n",
    "                                                                                                       \\end{pmatrix}\n",
    "                                                                                                       \\begin{pmatrix}\n",
    "                                                                                                       \\cos m \\theta_{i} \\newline\n",
    "                                                                                                       \\sin m \\theta_{i}\n",
    "                                                                                                       \\end{pmatrix} \\tag{40}\n",
    "$$\n",
    "$$\n",
    "\\boldsymbol{sec\\_row} \\ \\boldsymbol{x}_{i} = \\sin m \\theta_{i} \\ x_{i} + \\cos m \\theta_{i} \\ x'_{i} = \\begin{pmatrix}\n",
    "                                                                                                       x_{i} \\newline\n",
    "                                                                                                       x'_{i}\n",
    "                                                                                                       \\end{pmatrix}\n",
    "                                                                                                       \\begin{pmatrix}\n",
    "                                                                                                       \\sin m \\theta_{i} \\newline\n",
    "                                                                                                       \\cos m \\theta_{i}\n",
    "                                                                                                       \\end{pmatrix} \\tag{41}\n",
    "$$\n",
    "for the entire one, we can do like this:\n",
    "$$\n",
    "R_{\\Theta, m}^{d} \\ \\boldsymbol{x} = \\begin{pmatrix}\n",
    "                    x_{1} \\newline\n",
    "                    x_{1} \\newline\n",
    "                    x_{3} \\newline\n",
    "                    x_{4} \\newline\n",
    "                    \\vdots \\newline\n",
    "                    x_{d - 1} \\newline\n",
    "                    x_{d} \\newline\n",
    "                    \\end{pmatrix}\n",
    "                    \\otimes\n",
    "                    \\begin{pmatrix}\n",
    "                    \\cos m \\theta_{1} \\newline\n",
    "                    \\cos m \\theta_{1} \\newline\n",
    "                    \\cos m \\theta_{2} \\newline\n",
    "                    \\cos m \\theta_{2} \\newline\n",
    "                    \\vdots \\newline\n",
    "                    \\cos m \\theta_{d / 2} \\newline\n",
    "                    \\cos m \\theta_{d / 2} \\newline\n",
    "                    \\end{pmatrix}\n",
    "                    +\n",
    "                    \\begin{pmatrix}\n",
    "                    - x_{2} \\newline\n",
    "                    x_{1} \\newline\n",
    "                    - x_{4} \\newline\n",
    "                    x_{3} \\newline\n",
    "                    \\vdots \\newline\n",
    "                    - x_{d} \\newline\n",
    "                    x_{d - 1} \\newline\n",
    "                    \\end{pmatrix}\n",
    "                    \\otimes\n",
    "                    \\begin{pmatrix}\n",
    "                    \\sin m \\theta_{1} \\newline\n",
    "                    \\sin m \\theta_{1} \\newline\n",
    "                    \\sin m \\theta_{2} \\newline\n",
    "                    \\sin m \\theta_{2} \\newline\n",
    "                    \\vdots \\newline\n",
    "                    \\sin m \\theta_{d / 2} \\newline\n",
    "                    \\sin m \\theta_{d / 2} \\newline\n",
    "                    \\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5531f06c",
   "metadata": {},
   "source": [
    "<hr style=\"border: 0; height: 3px; background: linear-gradient(90deg, #ff0000, #00ff00); margin: 1em 0;\">\n",
    "Hoo, the math part ends.\n",
    "\n",
    "When implementing, we first give a function that wil go through how **RoPE** computing. Then we will encapsulate it into a class.\n",
    "\n",
    "And here we should note, how $ \\theta $ generate. Generally like this\n",
    "$$\n",
    "\\theta = \\frac{1}{base^{\\frac{2i}{dim}}}\n",
    "$$\n",
    "where $ i $ is the dimension index. Always we set $ base $ to $ 10000 $, so\n",
    "$$\n",
    "\\theta = \\frac{1}{10000^{\\frac{2i}{dim}}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "869e4a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RoPE_func(x, base=10000):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        x: input tensor, shape [batch_size, seq_len, num_heads, head_dim]\n",
    "           Note: Now you may not know what `num_heads` and `head_dim` is,\n",
    "                 it's not important now, you just need what the shape like\n",
    "        base: frequence base, default to 10000\n",
    "    \n",
    "    return:\n",
    "        the tensor applied RoPE, shape is the same with input\n",
    "\n",
    "    \"\"\"\n",
    "    # get shape info\n",
    "    batch_size, seq_len, num_heads, head_dim = x.shape\n",
    "\n",
    "    # generate position inidex (0 - seq_len - 1)\n",
    "    # shape: [seq_len]\n",
    "    position = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # generate dimension index (0 - head_dim / 2 - 1), you can analogy it to\n",
    "    # the R_{\\Theta, m}^{d}'s dimension, also the `d` in it.\n",
    "    dim = torch.arange(head_dim // 2, dtype=torch.float32, device=x.device)\n",
    "\n",
    "    # compute frequence parameter (theta_i)\n",
    "    # theta_i = 1 / (base^{2i / head_dim})\n",
    "    # shape: [head_dim // 2]\n",
    "    freq = 1.0 / (base ** (2 * dim / head_dim))\n",
    "\n",
    "    # compute each position and dimension's rotary angle\n",
    "    # m * theta_i where m is position index\n",
    "    # shape: [seq_len, head_dim // 2]\n",
    "    angles = position[:, None] * freq[None, :]\n",
    "\n",
    "    # compute cos and sin\n",
    "    cos = torch.cos(angles)\n",
    "    sin = torch.sin(angles)\n",
    "\n",
    "    # change shape for broadcasting, for x.shape = [batch_size, seq_len, num_heads, head_dim]\n",
    "    # [seq_len, head_dim // 2] ==> [1, seq_len, 1, head_dim // 2]\n",
    "    cos = cos[None, :, None, :]\n",
    "    sin = sin[None, :, None, :]\n",
    "    # we can also use `unsqueeze` function\n",
    "    # first convertion: [seq_len, head_dim // 2] ==> [1, seq_len, head_dim // 2]\n",
    "    # >>> cos = cos.unsqueeze(0)\n",
    "    # second convertion: [1, seq_len, head_dim // 2] ==> [1, seq_len, 1, head_dim // 2]\n",
    "    # >>> cos = cos.unsqueeze(2) \n",
    "    # so in all, we can do like this\n",
    "    # >>> cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    # so do with sin.\n",
    "    # another realization\n",
    "    # cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "    # sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    # split input tensor x to two subsection\n",
    "    # x0: even index section\n",
    "    # x1: odd index section\n",
    "    x0 = x[..., 0::2] # same with x = x[:, :, :, 0::2]\n",
    "    x1 = x[..., 1::2]\n",
    "\n",
    "    # apply rotary formula\n",
    "    # x0_rot = x0 * cos - x1 * sin | reference to Equation (40)\n",
    "    # x1_rot = x0 * sin + x2 * cos | reference to Equation (41)\n",
    "    x0_rot = x0 * cos - x1 * sin\n",
    "    x1_rot = x0 * sin + x1 * cos\n",
    "\n",
    "    # interleaved combination of the two parts after rotation\n",
    "    x_rot = torch.stack([x0_rot, x1_rot], dim=-1)\n",
    "    x_rot = x_rot.reshape(x.shape)\n",
    "\n",
    "    return x_rot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d425602f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    <h4>WARNING</h4>\n",
    "    <p>\n",
    "        <b>head_dim</b> must to be even, or unable to generate integer pairs of dimensions\n",
    "    </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "541da150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input shape: torch.Size([2, 10, 4, 64])\n",
      "Input:\n",
      " tensor([[[[-0.4219,  1.0965, -0.1230,  ..., -1.1313,  1.1512,  0.0749],\n",
      "          [-0.0478,  0.1124, -0.8396,  ..., -0.5118,  0.2506, -1.2593],\n",
      "          [ 0.5098, -0.2037, -0.3307,  ...,  0.6599,  1.4446, -0.3898],\n",
      "          [-0.2465, -1.2241, -1.7328,  ..., -0.2628,  0.0987,  0.4318]],\n",
      "\n",
      "         [[ 0.8173, -0.5064, -0.5369,  ..., -0.9785, -1.3159,  1.2071],\n",
      "          [-1.2622,  1.2874,  1.3678,  ..., -0.2201,  1.0068,  0.3634],\n",
      "          [-1.1442, -0.8554,  0.5581,  ..., -1.5964, -0.6175,  0.8353],\n",
      "          [-0.7543, -0.1788, -1.1366,  ...,  0.6146,  1.3140, -0.1259]],\n",
      "\n",
      "         [[-0.7148,  1.2496, -0.4481,  ..., -0.5338,  0.0083, -0.2982],\n",
      "          [ 0.5944, -0.6552, -0.7020,  ..., -1.2831,  1.6571, -0.2123],\n",
      "          [ 0.2385,  0.1594, -0.3189,  ...,  0.3165,  2.2159,  0.4421],\n",
      "          [ 0.0330, -0.6653,  1.0141,  ..., -0.0921, -0.7405,  1.4728]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.4080,  0.8711,  0.0590,  ..., -0.8908, -0.3482, -0.9261],\n",
      "          [ 0.3752, -1.4630,  0.9301,  ...,  2.6104,  0.7105, -0.7432],\n",
      "          [ 0.4309,  1.2086, -0.0483,  ..., -0.2614,  0.7180, -0.1293],\n",
      "          [-0.7886, -2.4144, -1.0819,  ..., -0.1810, -1.1552,  0.6904]],\n",
      "\n",
      "         [[-1.2679,  0.3416,  0.0324,  ...,  0.6953, -0.6914, -1.0547],\n",
      "          [ 0.5271, -0.3376,  0.6511,  ..., -0.5265, -1.4634, -1.3477],\n",
      "          [-0.8915,  0.0425, -0.5154,  ..., -1.1041, -1.5366, -0.9367],\n",
      "          [ 2.1423, -0.3921, -0.6320,  ..., -0.0036, -0.4850,  0.3354]],\n",
      "\n",
      "         [[ 0.3512,  0.6063,  0.6655,  ...,  0.0718,  1.2512, -0.2107],\n",
      "          [ 0.1810, -1.9615,  0.9426,  ..., -0.1120,  0.3721, -0.0483],\n",
      "          [-0.0317,  0.6578, -0.7803,  ..., -0.7911, -1.3325, -1.1113],\n",
      "          [-1.1600,  1.8980, -1.1962,  ..., -0.5967, -1.1916,  0.7821]]],\n",
      "\n",
      "\n",
      "        [[[-2.0326,  0.2099,  0.6387,  ...,  0.2174,  1.2168,  0.3940],\n",
      "          [-0.3427, -0.0184, -0.8271,  ...,  0.0976,  0.6341, -1.4046],\n",
      "          [ 0.0172,  1.6011, -2.8078,  ...,  0.6466, -1.7104, -0.7376],\n",
      "          [ 0.6111,  1.5054, -1.3951,  ...,  0.7224, -0.2948,  0.1884]],\n",
      "\n",
      "         [[ 0.5100,  1.7505, -1.9042,  ..., -0.8516,  0.3261, -1.8472],\n",
      "          [-0.2089,  0.2575, -0.5244,  ...,  1.0935,  1.4737, -0.4333],\n",
      "          [ 0.5831, -0.4454, -0.2281,  ..., -0.3907, -0.6412, -1.2931],\n",
      "          [-0.5197, -1.2300, -0.6879,  ...,  0.9591,  0.5029, -0.3098]],\n",
      "\n",
      "         [[-1.2715,  0.5688, -1.7550,  ..., -0.2123, -1.2970,  0.6005],\n",
      "          [-0.1128,  0.4612, -1.2985,  ...,  0.0687,  0.3433, -0.5318],\n",
      "          [ 0.5888,  0.9135, -1.2526,  ..., -0.8046, -1.1730,  0.5369],\n",
      "          [-0.1429, -1.2389,  0.8659,  ..., -0.4857, -0.8849,  1.3345]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.5947,  0.3361, -0.7133,  ..., -0.8608, -0.0521,  0.9515],\n",
      "          [ 1.5139, -0.6094,  1.1347,  ...,  0.4376, -0.8317, -0.5878],\n",
      "          [-1.1728,  0.8487,  0.8749,  ..., -0.1003,  0.5192,  1.2153],\n",
      "          [ 0.6669,  1.6408, -1.3419,  ..., -0.4451, -0.5844,  0.3911]],\n",
      "\n",
      "         [[ 2.3215, -0.4741,  0.7034,  ...,  0.4942,  0.3160, -0.2740],\n",
      "          [ 0.3245, -0.4007,  0.8709,  ..., -0.9113, -0.6750,  0.5307],\n",
      "          [ 0.6024,  1.3847, -0.2905,  ..., -1.1295, -1.8119,  0.5079],\n",
      "          [-1.3191, -0.6783,  0.6168,  ..., -0.6486,  2.2893,  0.6528]],\n",
      "\n",
      "         [[ 2.1556,  1.1607, -0.4304,  ...,  0.7609,  0.9149,  0.8235],\n",
      "          [-0.2524, -1.6486, -0.4872,  ...,  1.2758,  0.3901,  0.1865],\n",
      "          [-0.6726,  0.1349, -0.3134,  ...,  0.5743,  2.8600,  1.1076],\n",
      "          [-0.8183,  0.8482,  1.2581,  ...,  0.0091,  1.9446,  0.8608]]]])\n",
      "--------------------------------------------------\n",
      "Output shape: torch.Size([2, 10, 4, 64])\n",
      "Output:\n",
      " tensor([[[[-0.4219,  1.0965, -0.1230,  ..., -1.1313,  1.1512,  0.0749],\n",
      "          [-0.0478,  0.1124, -0.8396,  ..., -0.5118,  0.2506, -1.2593],\n",
      "          [ 0.5098, -0.2037, -0.3307,  ...,  0.6599,  1.4446, -0.3898],\n",
      "          [-0.2465, -1.2241, -1.7328,  ..., -0.2628,  0.0987,  0.4318]],\n",
      "\n",
      "         [[ 0.8677,  0.4141, -0.3652,  ..., -0.9784, -1.3161,  1.2070],\n",
      "          [-1.7653, -0.3665,  1.4866,  ..., -0.2199,  1.0068,  0.3635],\n",
      "          [ 0.1016, -1.4250,  0.8726,  ..., -1.5965, -0.6176,  0.8352],\n",
      "          [-0.2571, -0.7314, -0.2161,  ...,  0.6146,  1.3140, -0.1257]],\n",
      "\n",
      "         [[-0.8388, -1.1700,  0.9174,  ..., -0.5333,  0.0084, -0.2982],\n",
      "          [ 0.3484,  0.8131, -1.2118,  ..., -1.2831,  1.6572, -0.2118],\n",
      "          [-0.2442,  0.1506, -0.9121,  ...,  0.3164,  2.2158,  0.4426],\n",
      "          [ 0.5913,  0.3069,  0.2574,  ..., -0.0927, -0.7409,  1.4726]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.8799,  0.3886,  0.1540,  ..., -0.8897, -0.3473, -0.9265],\n",
      "          [ 1.2440, -0.8564,  0.2151,  ...,  2.6103,  0.7112, -0.7425],\n",
      "          [-0.4692,  1.1943, -0.8275,  ..., -0.2609,  0.7181, -0.1286],\n",
      "          [ 0.9917, -2.3383,  0.7080,  ..., -0.1802, -1.1558,  0.6894]],\n",
      "\n",
      "         [[-0.1535, -1.3041,  0.0323,  ...,  0.6953, -0.6903, -1.0555],\n",
      "          [ 0.2573,  0.5706,  0.1272,  ..., -0.5277, -1.4620, -1.3492],\n",
      "          [ 0.0877, -0.8882, -0.4639,  ..., -1.1052, -1.5356, -0.9383],\n",
      "          [ 0.0762,  2.1765, -0.4034,  ..., -0.0029, -0.4853,  0.3348]],\n",
      "\n",
      "         [[-0.5698, -0.4077,  0.7433,  ...,  0.0722,  1.2515, -0.2092],\n",
      "          [ 0.6434,  1.8618,  0.9874,  ..., -0.1107,  0.3721, -0.0479],\n",
      "          [-0.2422, -0.6124, -0.5785,  ..., -0.7905, -1.3312, -1.1129],\n",
      "          [ 0.2747, -2.2074, -1.2587,  ..., -0.5945, -1.1925,  0.7806]]],\n",
      "\n",
      "\n",
      "        [[[-2.0326,  0.2099,  0.6387,  ...,  0.2174,  1.2168,  0.3940],\n",
      "          [-0.3427, -0.0184, -0.8271,  ...,  0.0976,  0.6341, -1.4046],\n",
      "          [ 0.0172,  1.6011, -2.8078,  ...,  0.6466, -1.7104, -0.7376],\n",
      "          [ 0.6111,  1.5054, -1.3951,  ...,  0.7224, -0.2948,  0.1884]],\n",
      "\n",
      "         [[-1.1974,  1.3750, -1.3222,  ..., -0.8519,  0.3264, -1.8472],\n",
      "          [-0.3295, -0.0367,  0.1707,  ...,  1.0933,  1.4737, -0.4331],\n",
      "          [ 0.6898,  0.2500, -1.6800,  ..., -0.3909, -0.6410, -1.2932],\n",
      "          [ 0.7543, -1.1019, -1.0620,  ...,  0.9590,  0.5029, -0.3098]],\n",
      "\n",
      "         [[ 0.0120, -1.3929,  0.5710,  ..., -0.2124, -1.2972,  0.6001],\n",
      "          [-0.3725, -0.2945, -1.2392,  ...,  0.0685,  0.3435, -0.5317],\n",
      "          [-1.0756,  0.1552, -1.3955,  ..., -0.8040, -1.1732,  0.5366],\n",
      "          [ 1.1860,  0.3856, -0.5203,  ..., -0.4850, -0.8853,  1.3342]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2276,  0.6441,  1.7379,  ..., -0.8632, -0.0530,  0.9514],\n",
      "          [ 1.5417,  0.5352,  0.2227,  ...,  0.4392, -0.8312, -0.5886],\n",
      "          [-1.4417, -0.1307, -0.4886,  ..., -0.1010,  0.5181,  1.2158],\n",
      "          [-0.5752,  1.6751, -0.6983,  ..., -0.4448, -0.5847,  0.3906]],\n",
      "\n",
      "         [[ 0.1313,  2.3658,  0.5202,  ...,  0.4957,  0.3163, -0.2736],\n",
      "          [ 0.3492,  0.3794,  0.6293,  ..., -0.9103, -0.6756,  0.5300],\n",
      "          [-1.4576,  0.3945,  0.1583,  ..., -1.1280, -1.8125,  0.5060],\n",
      "          [ 0.8630, -1.2064,  0.9278,  ..., -0.6471,  2.2886,  0.6553]],\n",
      "\n",
      "         [[-2.4424, -0.1692,  0.1495,  ...,  0.7625,  0.9139,  0.8245],\n",
      "          [ 0.9094,  1.3981, -0.5153,  ...,  1.2761,  0.3898,  0.1870],\n",
      "          [ 0.5573, -0.4001, -0.7786,  ...,  0.5755,  2.8587,  1.1110],\n",
      "          [ 0.3961, -1.1100,  1.1965,  ...,  0.0113,  1.9435,  0.8631]]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "num_heads = 4\n",
    "head_dim = 64\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "x_rot = RoPE_func(x)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"-\" * 50)\n",
    "print(\"Output shape:\", x_rot.shape)\n",
    "print(\"Output:\\n\", x_rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9c676f",
   "metadata": {},
   "source": [
    "We encapsulate `rope` function to a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1918a7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim, base=10000):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            head_dim: head dimension must be even\n",
    "            base: frequency base\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # ensure head_dim is even\n",
    "        assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
    "        \n",
    "        self.head_dim = head_dim\n",
    "        self.base = base\n",
    "\n",
    "        # precompute theta_i\n",
    "        # shape: [head_dim // 2]\n",
    "        dim = torch.arange(head_dim // 2, dtype=torch.float32)\n",
    "        self.freq = 1.0 / (base ** (2 * dim / head_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input tensor, shape [bacth_size, seq_len, num_heads, head_dim]\n",
    "        \"\"\"\n",
    "        # get input tensor info\n",
    "        batch_size, seq_len, num_heads, head_dim = x.shape\n",
    "\n",
    "        # generate position index (0, seq_len - 1)\n",
    "        position = torch.arange(seq_len, dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # compute each position and dim's rotary angle\n",
    "        # m * theta_i, where m is position index\n",
    "        # shape: [seq_len, head_dim // 2]\n",
    "        angles = position[:, None] * self.freq[None, :].to(x.device)\n",
    "\n",
    "        # compute cos and sin\n",
    "        cos = torch.cos(angles)\n",
    "        sin = torch.sin(angles)\n",
    "\n",
    "        # reshape cos and sin for broadcasting\n",
    "        # below is another implemention\n",
    "        # >>> cos = cos.unsqueeze(0).unsqueeze(2)\n",
    "        # >>> sin = sin.unsqueeze(0).unsqueeze(2)\n",
    "        cos = cos[None, :, None, :]\n",
    "        sin = sin[None, :, None, :]\n",
    "\n",
    "        # split input to even index and odd index\n",
    "        x0 = x[..., 0::2]\n",
    "        x1 = x[..., 1::2]\n",
    "\n",
    "        # apply rotary formula\n",
    "        # x0_rot = x0 * cos - x1 * sin\n",
    "        # x1_rot = x0 * sin + x1 * cos\n",
    "        x0_rot = x0 * cos - x1 * sin\n",
    "        x1_rot = x0 * cos + x1 * sin\n",
    "\n",
    "        # interleaved combination of the two parts after rotation\n",
    "        x_rot = torch.stack([x0_rot, x1_rot], dim=-1) # on dimension head_dim\n",
    "        x_rot = x_rot.reshape(x.shape)\n",
    "\n",
    "        return x_rot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52e408ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Input shape: torch.Size([2, 10, 4, 64])\n",
      "Input:\n",
      " tensor([[[[-1.8583,  0.2212, -1.1290,  ..., -0.8682,  1.5962, -0.1677],\n",
      "          [-0.6703,  0.9843, -0.9771,  ...,  0.5599, -0.3622, -1.0328],\n",
      "          [-0.4438, -0.1608,  0.1577,  ..., -0.3047,  2.2329,  0.4616],\n",
      "          [-0.9899,  0.0988,  0.4164,  ..., -2.1633, -0.5897,  2.0057]],\n",
      "\n",
      "         [[-0.8417,  2.0367,  0.1312,  ...,  0.8909, -1.1462, -0.5552],\n",
      "          [ 0.0460,  0.9578,  0.3740,  ..., -0.9421,  0.4441, -0.3186],\n",
      "          [-1.9181,  1.4679, -1.3905,  ..., -0.5547, -0.0833, -0.9468],\n",
      "          [-0.8776, -0.3287,  0.0851,  ..., -0.5063,  0.0654, -1.4231]],\n",
      "\n",
      "         [[ 1.8964, -0.7096,  0.2442,  ..., -1.0329, -0.2469, -0.9901],\n",
      "          [ 0.8872, -0.1786, -0.9266,  ..., -0.1241,  0.5860, -1.4444],\n",
      "          [-1.9193,  0.6593,  2.2972,  ..., -2.0557, -0.4427, -0.5476],\n",
      "          [-0.2967, -0.6527, -0.3180,  ..., -0.3312,  1.0724,  1.0232]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.1574, -1.0002, -0.7124,  ..., -0.3415,  0.0605,  0.0519],\n",
      "          [ 1.2219, -0.4212,  0.6787,  ...,  0.4908,  0.1703, -0.8755],\n",
      "          [ 1.0229,  0.0705, -1.6477,  ..., -0.5974, -0.2786,  0.7435],\n",
      "          [ 0.4834, -0.5276, -0.5267,  ...,  0.3527,  0.7579,  0.1129]],\n",
      "\n",
      "         [[ 0.2807,  0.7289, -0.4080,  ..., -0.3793, -0.6663, -0.7998],\n",
      "          [ 0.6290,  0.4347,  1.2819,  ...,  2.5745,  0.0106,  0.3776],\n",
      "          [-1.0590, -0.2776,  1.5934,  ...,  0.2520,  0.5787, -0.0854],\n",
      "          [-0.2678, -1.0930,  0.0141,  ..., -0.3878, -0.1701, -0.1409]],\n",
      "\n",
      "         [[ 0.1394,  0.0149,  0.3221,  ...,  2.2545, -0.7331, -1.2286],\n",
      "          [ 0.5784,  0.5516, -1.5054,  ...,  2.8506,  0.2817, -0.4481],\n",
      "          [-0.9457, -1.8452,  0.9282,  ..., -0.1538, -2.0120, -1.2873],\n",
      "          [ 1.1586, -0.0518, -0.8324,  ...,  0.8999, -0.3398,  0.5010]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2414, -0.9807, -0.5153,  ...,  1.0394, -0.5088, -0.4457],\n",
      "          [-0.5994, -1.1512, -0.9218,  ..., -0.0981, -0.2498,  1.6242],\n",
      "          [-1.0502,  0.7575,  0.2899,  ..., -1.8033, -0.7582, -0.8967],\n",
      "          [ 1.9815, -0.2686,  0.3095,  ..., -0.3009, -0.4918, -1.4534]],\n",
      "\n",
      "         [[-2.2787, -0.1892,  0.3289,  ...,  1.2856,  0.5983,  0.4826],\n",
      "          [ 2.2527, -0.7768,  1.7636,  ..., -0.7487, -1.0222,  0.7750],\n",
      "          [ 0.9252,  0.3091, -1.2472,  ...,  0.3657,  0.2081, -0.7324],\n",
      "          [ 0.3195, -0.7746, -0.3272,  ...,  1.2903, -0.0958, -0.8252]],\n",
      "\n",
      "         [[ 1.1853, -1.1994,  0.1710,  ..., -0.9371, -1.0222,  0.3368],\n",
      "          [-1.7790,  0.4251,  1.4251,  ...,  0.7415, -0.3559,  1.1029],\n",
      "          [-0.4841,  0.2369, -2.1894,  ..., -1.1680,  0.3032, -1.1729],\n",
      "          [ 0.8450,  0.5964, -1.7902,  ..., -0.1521,  1.6344,  0.8946]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9479, -0.5837,  0.8446,  ...,  0.9845, -0.9690,  1.3597],\n",
      "          [ 0.4923, -0.4372,  0.2795,  ...,  0.3370, -2.3238, -0.3931],\n",
      "          [-0.1202, -2.0164,  0.4339,  ..., -0.3615,  0.0697, -0.3751],\n",
      "          [ 0.8364, -0.1934,  0.2056,  ...,  0.1660,  0.4675,  0.4337]],\n",
      "\n",
      "         [[-1.4513, -0.2231,  0.5672,  ...,  0.5001,  0.1694,  0.6325],\n",
      "          [ 0.2885,  0.1931, -0.6635,  ...,  0.1451,  0.1504,  0.6400],\n",
      "          [ 0.5999,  1.9471, -0.6691,  ...,  0.8361, -0.1740, -0.3857],\n",
      "          [-0.4815, -1.5407,  0.7982,  ...,  0.0387,  0.2941,  0.1316]],\n",
      "\n",
      "         [[ 0.0075, -0.3737, -0.5918,  ..., -1.9528, -0.0374, -1.8226],\n",
      "          [ 0.4216, -0.9226, -0.3043,  ...,  0.7377,  0.1599, -0.7414],\n",
      "          [-1.9805,  0.3828,  1.2360,  ...,  0.3877,  0.5066, -0.1307],\n",
      "          [ 0.0899, -0.2582,  1.1245,  ...,  0.7821,  1.3495, -0.5335]]]])\n",
      "--------------------------------------------------\n",
      "Output shape: torch.Size([2, 10, 4, 64])\n",
      "Output:\n",
      " tensor([[[[-1.8583, -1.8583, -1.1290,  ...,  0.3822,  1.5962,  1.5962],\n",
      "          [-0.6703, -0.6703, -0.9771,  ..., -0.8385, -0.3622, -0.3622],\n",
      "          [-0.4438, -0.4438,  0.1577,  ...,  1.9549,  2.2329,  2.2329],\n",
      "          [-0.9899, -0.9899,  0.4164,  ..., -0.7816, -0.5897, -0.5897]],\n",
      "\n",
      "         [[-2.1686,  1.2591, -0.9275,  ..., -0.2784, -1.1461, -1.1462],\n",
      "          [-0.7811,  0.8308,  0.3422,  ...,  0.4596,  0.4441,  0.4440],\n",
      "          [-2.2715,  0.1988, -1.2535,  ...,  0.6340, -0.0832, -0.0834],\n",
      "          [-0.1976, -0.7508,  0.3638,  ...,  0.4448,  0.0656,  0.0652]],\n",
      "\n",
      "         [[-0.1439, -1.4345, -0.1229,  ..., -0.4080, -0.2466, -0.2471],\n",
      "          [-0.2068, -0.5316,  0.0473,  ...,  2.9831,  0.5863,  0.5856],\n",
      "          [ 0.1992,  1.3982, -0.2234,  ..., -0.5655, -0.4426, -0.4429],\n",
      "          [ 0.7170, -0.4701, -1.0778,  ...,  0.8848,  1.0722,  1.0727]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.7757, -0.5385,  1.0052,  ..., -0.2820,  0.0605,  0.0606],\n",
      "          [ 1.1980,  0.6445,  0.3463,  ..., -0.4337,  0.1711,  0.1695],\n",
      "          [ 0.7248,  0.8175,  0.0096,  ...,  0.9365, -0.2793, -0.2779],\n",
      "          [ 0.7111,  0.0178,  0.7739,  ...,  0.3618,  0.7578,  0.7580]],\n",
      "\n",
      "         [[-0.7620,  0.6803, -0.1337,  ...,  0.2446, -0.6654, -0.6672],\n",
      "          [-0.5216,  0.3385,  1.5195,  ..., -1.1014,  0.0102,  0.0111],\n",
      "          [ 0.4287, -0.1206,  1.0768,  ...,  0.8128,  0.5788,  0.5786],\n",
      "          [ 1.1203, -1.0424,  0.6751,  ...,  1.3259, -0.1699, -0.1702]],\n",
      "\n",
      "         [[-0.1332, -0.1209,  0.1082,  ..., -0.1826, -0.7316, -0.7346],\n",
      "          [-0.7543, -0.2997, -1.9247,  ...,  0.4539,  0.2822,  0.2811],\n",
      "          [ 1.6221,  0.1012,  1.6587,  ..., -1.3582, -2.0105, -2.0136],\n",
      "          [-1.0343, -1.0770,  0.2930,  ..., -2.0131, -0.3404, -0.3392]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2414,  1.2414, -0.5153,  ..., -0.1787, -0.5088, -0.5088],\n",
      "          [-0.5994, -0.5994, -0.9218,  ..., -0.3536, -0.2498, -0.2498],\n",
      "          [-1.0502, -1.0502,  0.2899,  ..., -0.3414, -0.7582, -0.7582],\n",
      "          [ 1.9815,  1.9815,  0.3095,  ..., -0.1153, -0.4918, -0.4918]],\n",
      "\n",
      "         [[-1.0720, -1.3904, -0.1540,  ..., -2.2910,  0.5983,  0.5984],\n",
      "          [ 1.8708,  0.5635,  1.5455,  ..., -0.4383, -1.0223, -1.0221],\n",
      "          [ 0.2398,  0.7600, -0.3905,  ...,  0.4165,  0.2082,  0.2080],\n",
      "          [ 0.8244, -0.4792,  0.1111,  ...,  1.8405, -0.0957, -0.0959]],\n",
      "\n",
      "         [[ 0.5973, -1.5839,  0.2071,  ...,  1.1187, -1.0223, -1.0221],\n",
      "          [ 0.3538,  1.1268, -0.4275,  ...,  0.4262, -0.3562, -0.3556],\n",
      "          [-0.0140,  0.4169, -0.2443,  ..., -0.4650,  0.3036,  0.3029],\n",
      "          [-0.8940,  0.1907, -1.2382,  ...,  2.4540,  1.6342,  1.6347]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0981,  0.3312, -0.5679,  ...,  0.6271, -0.9702, -0.9677],\n",
      "          [ 0.6584,  0.0840, -0.4064,  ...,  1.7506, -2.3235, -2.3242],\n",
      "          [ 1.2341, -1.4154,  0.8604,  ...,  0.6656,  0.0700,  0.0693],\n",
      "          [ 0.7577,  0.5035, -2.0578,  ...,  0.0767,  0.4671,  0.4679]],\n",
      "\n",
      "         [[ 0.4318, -0.0095,  0.3494,  ..., -0.1819,  0.1687,  0.1701],\n",
      "          [-0.2330,  0.1491, -1.1019,  ..., -0.9914,  0.1497,  0.1511],\n",
      "          [-2.0136,  1.8391, -0.9856,  ...,  0.3725, -0.1736, -0.1744],\n",
      "          [ 1.5944, -1.4542,  0.9650,  ...,  1.2134,  0.2939,  0.2942]],\n",
      "\n",
      "         [[ 0.1472, -0.1608, -0.3924,  ...,  1.4315, -0.0353, -0.0396],\n",
      "          [-0.0040, -0.7644,  0.2080,  ..., -0.5701,  0.1608,  0.1590],\n",
      "          [ 1.6468,  1.9623,  1.6526,  ..., -0.7983,  0.5067,  0.5064],\n",
      "          [ 0.0245, -0.1883,  1.2556,  ...,  1.2455,  1.3502,  1.3489]]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "seq_len = 10\n",
    "num_heads = 4\n",
    "head_dim = 64\n",
    "\n",
    "rope = RoPE(head_dim=head_dim, base=10000)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "# apply RoPE\n",
    "x_rot = rope.forward(x)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Input:\\n\", x)\n",
    "print(\"-\" * 50)\n",
    "print(\"Output shape:\", x_rot.shape)\n",
    "print(\"Output:\\n\", x_rot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948b350",
   "metadata": {},
   "source": [
    "> [!ATTENTION]\n",
    ">\n",
    "> `x_rot` is just position embedding, so you need to plus the original input `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6d42c87d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed_input shape: torch.Size([2, 10, 4, 64])\n",
      "embed_input:\n",
      " tensor([[[[-3.7166, -1.6371, -2.2580,  ..., -0.4860,  3.1925,  1.4285],\n",
      "          [-1.3406,  0.3140, -1.9542,  ..., -0.2787, -0.7244, -1.3950],\n",
      "          [-0.8875, -0.6046,  0.3155,  ...,  1.6502,  4.4657,  2.6944],\n",
      "          [-1.9798, -0.8911,  0.8328,  ..., -2.9449, -1.1795,  1.4160]],\n",
      "\n",
      "         [[-3.0102,  3.2958, -0.7963,  ...,  0.6125, -2.2923, -1.7014],\n",
      "          [-0.7351,  1.7887,  0.7162,  ..., -0.4825,  0.8882,  0.1254],\n",
      "          [-4.1897,  1.6667, -2.6439,  ...,  0.0793, -0.1665, -1.0302],\n",
      "          [-1.0752, -1.0795,  0.4489,  ..., -0.0616,  0.1309, -1.3579]],\n",
      "\n",
      "         [[ 1.7525, -2.1441,  0.1212,  ..., -1.4409, -0.4935, -1.2372],\n",
      "          [ 0.6804, -0.7102, -0.8793,  ...,  2.8591,  1.1723, -0.8588],\n",
      "          [-1.7201,  2.0575,  2.0738,  ..., -2.6212, -0.8853, -0.9904],\n",
      "          [ 0.4203, -1.1228, -1.3958,  ...,  0.5535,  2.1446,  2.0959]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9331, -1.5387,  0.2928,  ..., -0.6235,  0.1210,  0.1125],\n",
      "          [ 2.4199,  0.2232,  1.0249,  ...,  0.0572,  0.3414, -0.7060],\n",
      "          [ 1.7478,  0.8881, -1.6380,  ...,  0.3392, -0.5579,  0.4656],\n",
      "          [ 1.1946, -0.5098,  0.2472,  ...,  0.7145,  1.5156,  0.8709]],\n",
      "\n",
      "         [[-0.4813,  1.4092, -0.5416,  ..., -0.1346, -1.3317, -1.4669],\n",
      "          [ 0.1074,  0.7732,  2.8014,  ...,  1.4732,  0.0209,  0.3887],\n",
      "          [-0.6303, -0.3982,  2.6702,  ...,  1.0648,  1.1575,  0.4933],\n",
      "          [ 0.8525, -2.1354,  0.6892,  ...,  0.9381, -0.3400, -0.3112]],\n",
      "\n",
      "         [[ 0.0062, -0.1059,  0.4303,  ...,  2.0719, -1.4647, -1.9631],\n",
      "          [-0.1759,  0.2519, -3.4301,  ...,  3.3046,  0.5639, -0.1670],\n",
      "          [ 0.6764, -1.7440,  2.5869,  ..., -1.5119, -4.0225, -3.3008],\n",
      "          [ 0.1243, -1.1288, -0.5394,  ..., -1.1132, -0.6802,  0.1618]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4828,  0.2607, -1.0307,  ...,  0.8607, -1.0175, -0.9545],\n",
      "          [-1.1987, -1.7506, -1.8436,  ..., -0.4516, -0.4995,  1.3744],\n",
      "          [-2.1004, -0.2928,  0.5799,  ..., -2.1447, -1.5163, -1.6549],\n",
      "          [ 3.9630,  1.7129,  0.6190,  ..., -0.4162, -0.9837, -1.9452]],\n",
      "\n",
      "         [[-3.3506, -1.5796,  0.1750,  ..., -1.0054,  1.1966,  1.0810],\n",
      "          [ 4.1234, -0.2133,  3.3091,  ..., -1.1870, -2.0444, -0.2471],\n",
      "          [ 1.1650,  1.0690, -1.6377,  ...,  0.7822,  0.4163, -0.5244],\n",
      "          [ 1.1439, -1.2538, -0.2161,  ...,  3.1308, -0.1915, -0.9211]],\n",
      "\n",
      "         [[ 1.7827, -2.7833,  0.3781,  ...,  0.1816, -2.0444, -0.6852],\n",
      "          [-1.4252,  1.5519,  0.9975,  ...,  1.1677, -0.7120,  0.7473],\n",
      "          [-0.4981,  0.6538, -2.4338,  ..., -1.6330,  0.6068, -0.8700],\n",
      "          [-0.0490,  0.7872, -3.0283,  ...,  2.3018,  3.2686,  2.5293]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.0461, -0.2525,  0.2767,  ...,  1.6115, -1.9392,  0.3920],\n",
      "          [ 1.1507, -0.3532, -0.1270,  ...,  2.0876, -4.6473, -2.7173],\n",
      "          [ 1.1139, -3.4318,  1.2943,  ...,  0.3041,  0.1397, -0.3057],\n",
      "          [ 1.5941,  0.3101, -1.8522,  ...,  0.2427,  0.9346,  0.9016]],\n",
      "\n",
      "         [[-1.0195, -0.2326,  0.9165,  ...,  0.3182,  0.3381,  0.8026],\n",
      "          [ 0.0555,  0.3421, -1.7653,  ..., -0.8463,  0.3002,  0.7911],\n",
      "          [-1.4137,  3.7861, -1.6548,  ...,  1.2086, -0.3476, -0.5601],\n",
      "          [ 1.1128, -2.9949,  1.7632,  ...,  1.2521,  0.5880,  0.4258]],\n",
      "\n",
      "         [[ 0.1547, -0.5346, -0.9842,  ..., -0.5213, -0.0727, -1.8622],\n",
      "          [ 0.4177, -1.6869, -0.0963,  ...,  0.1676,  0.3207, -0.5824],\n",
      "          [-0.3337,  2.3450,  2.8886,  ..., -0.4106,  1.0133,  0.3758],\n",
      "          [ 0.1144, -0.4466,  2.3801,  ...,  2.0276,  2.6997,  0.8154]]]])\n"
     ]
    }
   ],
   "source": [
    "embed_input = x + x_rot\n",
    "\n",
    "print(\"embed_input shape:\", embed_input.shape)\n",
    "print(\"embed_input:\\n\", embed_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394bf24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3 Loading pretrained embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8085e5e5",
   "metadata": {},
   "source": [
    "Here must to indicate: here we use above three embedding methods, but sometimes we will use `Word2Vec`, `GloVe` etc.\n",
    "\n",
    "After embedding, we must to build a model, and train it. It would connect to `transformer` and other things. Here we don't do this, we just use the pretrained one.\n",
    "\n",
    "Do you remember download one pretrained tokenzier? We will use its embedding also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d5acf37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModel\n",
    "# before we imported AutoTokenizer, so hear, we just import AutoModel\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957a146",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    <h4>Note</h4>\n",
    "    <p>\n",
    "        Below <code>embeddings</code> don't have <b>position embedding</b>, its just <b>token embedding</b>. <code>RoPE</code> is defined in the <b>transformer</b>'s forward function.\n",
    "    </p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44abaa63",
   "metadata": {},
   "source": [
    "Download the model. Code below can resume from breakpoint, so you can interupt it anytime then start it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "78ba9912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'hold', 'you', 'and', 'you', 'hold', 'me']\n",
      "'hold' appears at positions: [1, 5]\n",
      "\n",
      "Token embeddings (no position info):\n",
      "tensor([ 0.0459,  0.0172, -0.0317,  0.0264, -0.0166], device='cuda:0',\n",
      "       dtype=torch.float16, grad_fn=<SliceBackward0>)\n",
      "\n",
      "Final hidden states (with position info):\n",
      "tensor([-1.0303, -3.3359,  2.7383,  1.6855,  1.1680], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "\n",
      "Difference between positions:\n",
      "Mean absolute difference: 1.92578125\n",
      "First 5 differences: [0.24169921875, 2.48828125, 2.078125, 3.12890625, 0.12890625]\n"
     ]
    }
   ],
   "source": [
    "cachedir = \"./deepseek-1.5B\"\n",
    "modelname = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "text = \"I hold you and you hold me\"\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname, cache_dir=cachedir)\n",
    "model = AutoModel.from_pretrained(modelname, cache_dir=cachedir, device_map=device if device.startswith(\"cuda\") else \"cpu\",\n",
    "    torch_dtype=torch.float16 if device.startswith(\"cuda\") else torch.float32)\n",
    "embeddings = model.get_input_embeddings()\n",
    "\n",
    "# Tokenize the text\n",
    "# tokens: ['I', 'hold', 'you', 'and', 'you', 'hold', 'me']\n",
    "# Note: '' stands for space here\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Find all positions of the token \"hold\" (note the space prefix)\n",
    "hold_positions = [i for i, token in enumerate(tokens) if token == \"hold\"]\n",
    "print(\"'hold' appears at positions:\", hold_positions)\n",
    "\n",
    "# Check if we found any occurrences\n",
    "if not hold_positions:\n",
    "    print(\"Error: 'hold' token not found in tokenized text\")\n",
    "    exit()\n",
    "\n",
    "# Get token embeddings without position information\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "token_embeddings = embeddings(inputs[\"input_ids\"])\n",
    "print(\"\\nToken embeddings (no position info):\")\n",
    "print(token_embeddings[0, hold_positions[0], :5])  # First occurrence of \"hold\"\n",
    "\n",
    "# Get model output (with position information)\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"], output_hidden_states=True)\n",
    "\n",
    "# Get the last hidden state (with position information)\n",
    "final_hidden_states = outputs.last_hidden_state\n",
    "print(\"\\nFinal hidden states (with position info):\")\n",
    "print(final_hidden_states[0, hold_positions[0], :5])  # First occurrence of \"hold\"\n",
    "\n",
    "# Compare the same token at different positions\n",
    "if len(hold_positions) >= 2:\n",
    "    print(\"\\nDifference between positions:\")\n",
    "    diff = torch.abs(final_hidden_states[0, hold_positions[0]] - final_hidden_states[0, hold_positions[1]])\n",
    "    print(\"Mean absolute difference:\", diff.mean().item())\n",
    "    print(\"First 5 differences:\", diff[:5].tolist())\n",
    "else:\n",
    "    print(\"\\nNot enough occurrences of 'hold' to compare positions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
