{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f99f53",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2047ee5",
   "metadata": {},
   "source": [
    "This chapter may be the most important part of the whole project. **Transformer** is the core in LLM. In **transformer**, **attention** is the core. It change the way we regard word also the way we train our model. Before we usually use **RNN** rather its linear, you must compute the former, then you could compute latter. That's a waste of computer resources. For computer in most cases, tasks can be performed in parallel, especially on **GPU**.\n",
    "\n",
    "**Transformer** is based on matrix operations which can be highly parallelizable. So when we train model, ususally on **GPU**.\n",
    "\n",
    "We will begin with mechanism of transformer then we implement original one and variantions in code. Next we will try PyTorch's implemention of transformer. Finally we will end with the comparison between **RNN**, **transformer** and different implementions of **transformer**.\n",
    "\n",
    "> [!NOTE]\n",
    ">\n",
    "> original paper of [transformer](https://arxiv.org/pdf/1706.03762)\n",
    "\n",
    "**Attention is all your need**. Let's begin!\n",
    "\n",
    "<hr style=\"border: 0; height: 3px; background: linear-gradient(90deg, #ff0000, #00ff00); margin: 1em 0;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1513a48e",
   "metadata": {},
   "source": [
    "## 1 Attention\n",
    "\n",
    "Consider we have a *input text*: `Attention is all your need`. In **transformer** we will first intialize three matrix $ W_{q} $, $ W_{k} $, $ W_{v} $ . In it, $ q $, $ k $, $ v $ stands for `query`, `key`, `value` respectively. We will use embedding converting *input text* to a vector. Then we use three matrices to respectively multiply this vector that will generates three vectors: `query`, `key` and `value`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60af5957",
   "metadata": {},
   "source": [
    "## 2 MHA(Multi-Head Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbf0fa6",
   "metadata": {},
   "source": [
    "## 3 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1a1de0",
   "metadata": {},
   "source": [
    "## 4 Variantions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de2cd1",
   "metadata": {},
   "source": [
    "## 5 PyTorch's Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fee65",
   "metadata": {},
   "source": [
    "## 6 Comparision\n",
    "\n",
    "### 6.1 RNN and transformer\n",
    "\n",
    "### 6.2 transformer and transformer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
