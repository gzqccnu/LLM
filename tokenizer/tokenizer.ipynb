{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0b2dfaa",
   "metadata": {},
   "source": [
    "# Tokenzier\n",
    "\n",
    "What is tokenizer?\n",
    "<br>\n",
    "Actually, by my understanding, it's just a directionary with index. By the unique index, we can find the word so as we called token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d772f",
   "metadata": {},
   "source": [
    "## 0. Build a dictionary\n",
    "Here, if we have sentence \"I love you LLM.\". By what, We can have a index, and use the index to find the unique word.\n",
    "<br>\n",
    "Actually, we can use the directionary data structure. One word has the only number to direct it. And we call the number(index) key,\n",
    "and the word as value.\n",
    "<br>\n",
    "So, the dictionary is just the key-value pair.\n",
    "<br>\n",
    "Now, let's build one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a4a68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1  \n",
      "2 l\n",
      "3 o\n",
      "4 v\n",
      "5 e\n",
      "6  \n",
      "7 l\n",
      "8 e\n",
      "9 a\n",
      "10 r\n",
      "11 n\n",
      "12 i\n",
      "13 n\n",
      "14 g\n",
      "15  \n",
      "16 L\n",
      "17 L\n",
      "18 M\n",
      "19 .\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"I love learning LLM.\"\n",
    "index = 0\n",
    "# In python, {} is a directionary data structure.\n",
    "dict = {}\n",
    "\n",
    "for char in raw_text:\n",
    "    dict[index] = char\n",
    "    index += 1\n",
    "\n",
    "for key, value in dict.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974703fe",
   "metadata": {},
   "source": [
    "We do this, and we got the every character in the text, not word, so we should use some method to get word instead.\n",
    "<br>\n",
    "We can use the `split()` function. That will do the thing for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7c0976",
   "metadata": {},
   "source": [
    "### 0.1 split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91af3c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 love\n",
      "2 learning\n",
      "3 LLM.\n"
     ]
    }
   ],
   "source": [
    "text = raw_text.split(' ')\n",
    "index = 0\n",
    "dict = {}\n",
    "\n",
    "for word in text:\n",
    "    dict[index] = word\n",
    "    index += 1\n",
    "\n",
    "for (index, word) in dict.items():\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab973e4",
   "metadata": {},
   "source": [
    "And we will see, the last one, \"LLM.\" that would not be spit. That's not what we want.\n",
    "<br>\n",
    "So we introduce the regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27043f2",
   "metadata": {},
   "source": [
    "### 0.2 Regular Expression\n",
    "We often call Regular Expression as **re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07449bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba71961b",
   "metadata": {},
   "source": [
    "And we can split the text with re.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8299db",
   "metadata": {},
   "source": [
    "It requires pattern and string.\n",
    "<br>\n",
    "And the `\\s` pattern stands for split character like blank, and '\\t' etc. But it don't contain period.\n",
    "<br>\n",
    "But re offers better use, you can assign it in pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6f8976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I\n",
      "1 love\n",
      "2 learning\n",
      "3 LLM\n",
      "4 \n"
     ]
    }
   ],
   "source": [
    "text = re.split('\\s|\\.', raw_text)\n",
    "dict = {}\n",
    "index = 0\n",
    "\n",
    "for word in text:\n",
    "    dict[index] = word\n",
    "    index += 1\n",
    "\n",
    "for (index, word) in dict.items():\n",
    "    print(index, word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374ab05d",
   "metadata": {},
   "source": [
    "And now we can use index to Visit these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "818b0cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index=0, word=I\n",
      "index=1, word=love\n",
      "index=2, word=learning\n",
      "index=3, word=LLM\n"
     ]
    }
   ],
   "source": [
    "for i in range(index):\n",
    "    print(f\"index={i}, word={dict[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4049d358",
   "metadata": {},
   "source": [
    "Here, we finish it.\n",
    "<br>\n",
    "And let's do another one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff30cb2",
   "metadata": {},
   "source": [
    "### 0.3 Convert a longer corpus to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33887f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../corpus/poetry.txt') as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924a527e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text length: 1215\n"
     ]
    }
   ],
   "source": [
    "print(\"Text length:\", len(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0104d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can I hold you with?\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53114b54",
   "metadata": {},
   "source": [
    "And let's do the same thing to split it.\n",
    "<br>\n",
    "Attention: in the re\n",
    "- the pattern can be rounded with `()`, it's called a group\n",
    "- the pattern can be rouded with `[]` means if there one character in the `[]` occurs in object string, and it will matched. Even in the `[]` may have many characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ec2a8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', ' ', 'can', ' ', 'I', ' ', 'hold', ' ', 'you', ' ', 'with', '?']\n"
     ]
    }
   ],
   "source": [
    "pretext = re.split('([.,\\'\\\"?!:;()]|--|\\s)', raw_text)\n",
    "\n",
    "print(pretext[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0beb7ea",
   "metadata": {},
   "source": [
    "That's it, we accomplished half one of a tokenzier, we called decode\n",
    "<br>\n",
    "What decode do is just to convert a index or word id to a word\n",
    "<br>\n",
    "The another half is called encode. It's the oppsite operation. What encode do is just to convert a text or corpus to a list of index or word id and convenient our computation in computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dabb060",
   "metadata": {},
   "source": [
    "---\n",
    "**Now we formally deep into the tokenizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16e646",
   "metadata": {},
   "source": [
    "## 1 Encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4334a",
   "metadata": {},
   "source": [
    "We still use the poetry as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a673259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can I hold you with?\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965f12e3",
   "metadata": {},
   "source": [
    "Here we can direcly use the `pretext` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d4d4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', ' ', 'can', ' ', 'I', ' ', 'hold', ' ', 'you', ' ', 'with', '?']\n"
     ]
    }
   ],
   "source": [
    "print(pretext[:12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dddc62",
   "metadata": {},
   "source": [
    "As what we state above: encode is to convert a text or corpus to a list of index. So it's just a dictionary of (word, id). And we actully call the word as **token**, because in the original text we are not only have word, but also punctuation mark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe76e866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('What', 0)\n",
      "(' ', 527)\n",
      "('can', 472)\n",
      "('I', 502)\n",
      "('hold', 260)\n",
      "('you', 512)\n",
      "('with', 526)\n",
      "('?', 11)\n",
      "('', 530)\n",
      "('\\n', 469)\n",
      "('offer', 434)\n",
      "('lean', 22)\n",
      "('streets', 24)\n",
      "(',', 523)\n",
      "('desperate', 28)\n",
      "('sunsets', 30)\n",
      "('the', 490)\n",
      "('moon', 84)\n",
      "('of', 494)\n",
      "('jagged', 42)\n",
      "('suburbs', 44)\n",
      "('.', 529)\n",
      "('bitterness', 58)\n",
      "('a', 404)\n",
      "('man', 294)\n",
      "('who', 296)\n",
      "('has', 298)\n",
      "('looked', 70)\n",
      "('long', 76)\n",
      "('and', 456)\n",
      "('at', 412)\n",
      "('lonely', 82)\n",
      "('my', 496)\n",
      "('ancestors', 98)\n",
      "('dead', 170)\n",
      "('men', 222)\n",
      "('ghosts', 232)\n",
      "('that', 344)\n",
      "('living', 116)\n",
      "('have', 328)\n",
      "('honoured', 122)\n",
      "('in', 350)\n",
      "('bronze', 126)\n",
      "(':', 127)\n",
      "('father', 136)\n",
      "(\"'\", 199)\n",
      "('s', 200)\n",
      "('killed', 138)\n",
      "('frontier', 144)\n",
      "('Buenos', 148)\n",
      "('Aires', 150)\n",
      "('two', 154)\n",
      "('bullets', 156)\n",
      "('through', 158)\n",
      "('his', 178)\n",
      "('lungs', 162)\n",
      "('bearded', 166)\n",
      "('wrapped', 174)\n",
      "('by', 384)\n",
      "('soldiers', 180)\n",
      "('hide', 186)\n",
      "('cow', 192)\n",
      "(';', 499)\n",
      "('mother', 198)\n",
      "('grandfather', 202)\n",
      "('--', 337)\n",
      "('just', 206)\n",
      "('twentyfour', 208)\n",
      "('heading', 210)\n",
      "('charge', 214)\n",
      "('three', 218)\n",
      "('hundred', 220)\n",
      "('Peru', 226)\n",
      "('now', 230)\n",
      "('on', 234)\n",
      "('vanished', 236)\n",
      "('horses', 238)\n",
      "('whatever', 264)\n",
      "('insight', 252)\n",
      "('books', 256)\n",
      "('may', 258)\n",
      "('manliness', 266)\n",
      "('or', 268)\n",
      "('humour', 270)\n",
      "('life', 274)\n",
      "('loyalty', 288)\n",
      "('never', 300)\n",
      "('been', 302)\n",
      "('loyal', 304)\n",
      "('kernel', 318)\n",
      "('myself', 322)\n",
      "('saved', 330)\n",
      "('somehow', 334)\n",
      "('central', 340)\n",
      "('heart', 498)\n",
      "('deals', 346)\n",
      "('not', 358)\n",
      "('words', 352)\n",
      "('traffics', 356)\n",
      "('dreams', 362)\n",
      "('is', 368)\n",
      "('untouched', 370)\n",
      "('time', 374)\n",
      "('joy', 380)\n",
      "('adversities', 386)\n",
      "('memory', 400)\n",
      "('yellow', 406)\n",
      "('rose', 408)\n",
      "('seen', 410)\n",
      "('sunset', 414)\n",
      "('years', 418)\n",
      "('before', 420)\n",
      "('were', 424)\n",
      "('born', 426)\n",
      "('explanations', 438)\n",
      "('yourself', 464)\n",
      "('theories', 446)\n",
      "('about', 448)\n",
      "('authentic', 454)\n",
      "('surprising', 458)\n",
      "('news', 460)\n",
      "('give', 474)\n",
      "('loneliness', 480)\n",
      "('darkness', 486)\n",
      "('hunger', 492)\n",
      "('am', 504)\n",
      "('trying', 506)\n",
      "('to', 508)\n",
      "('bribe', 510)\n",
      "('uncertainty', 516)\n",
      "('danger', 522)\n",
      "('defeat', 528)\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "index = 0\n",
    "\n",
    "for token in pretext:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "\n",
    "for token, index in vocab.items():\n",
    "    print((token, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03a3b2",
   "metadata": {},
   "source": [
    "And we see: the index is not increasing, cause the token in the corpus will always be repeat. So you will note that some index is disapeer.\n",
    "That's for the repeat. So we must remove it. So we use **set**. It's another data structure. In set will not occur repeat elem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0efaee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('', 0)\n",
      "('bullets', 1)\n",
      "('am', 2)\n",
      "('books', 3)\n",
      "('with', 4)\n",
      "('danger', 5)\n",
      "('memory', 6)\n",
      "('saved', 7)\n",
      "('who', 8)\n",
      "('yellow', 9)\n",
      "(\"'\", 10)\n",
      "('joy', 11)\n",
      "('never', 12)\n",
      "('in', 13)\n",
      "('dead', 14)\n",
      "('Aires', 15)\n",
      "('sunset', 16)\n",
      "('give', 17)\n",
      "('on', 18)\n",
      "('father', 19)\n",
      "('by', 20)\n",
      "('darkness', 21)\n",
      "('not', 22)\n",
      "('time', 23)\n",
      "('uncertainty', 24)\n",
      "('\\n', 25)\n",
      "('desperate', 26)\n",
      "('of', 27)\n",
      "('the', 28)\n",
      "('s', 29)\n",
      "('untouched', 30)\n",
      "('before', 31)\n",
      "('heading', 32)\n",
      "('hold', 33)\n",
      "('have', 34)\n",
      "('looked', 35)\n",
      "('moon', 36)\n",
      "('two', 37)\n",
      "('his', 38)\n",
      "('vanished', 39)\n",
      "('kernel', 40)\n",
      "('been', 41)\n",
      "('sunsets', 42)\n",
      "('seen', 43)\n",
      "('hunger', 44)\n",
      "(',', 45)\n",
      "('?', 46)\n",
      "('lungs', 47)\n",
      "('news', 48)\n",
      "('ancestors', 49)\n",
      "('I', 50)\n",
      "('words', 51)\n",
      "('bribe', 52)\n",
      "('life', 53)\n",
      "('now', 54)\n",
      "('hide', 55)\n",
      "('insight', 56)\n",
      "('twentyfour', 57)\n",
      "('horses', 58)\n",
      "('myself', 59)\n",
      "('--', 60)\n",
      "('Buenos', 61)\n",
      "('somehow', 62)\n",
      "('rose', 63)\n",
      "('authentic', 64)\n",
      "('man', 65)\n",
      "('central', 66)\n",
      "('What', 67)\n",
      "('lonely', 68)\n",
      "('cow', 69)\n",
      "('men', 70)\n",
      "('and', 71)\n",
      "('may', 72)\n",
      "('loneliness', 73)\n",
      "('mother', 74)\n",
      "('at', 75)\n",
      "('loyal', 76)\n",
      "('explanations', 77)\n",
      "('theories', 78)\n",
      "('honoured', 79)\n",
      "('hundred', 80)\n",
      "('charge', 81)\n",
      "('were', 82)\n",
      "('surprising', 83)\n",
      "('three', 84)\n",
      "('offer', 85)\n",
      "('Peru', 86)\n",
      "('that', 87)\n",
      "('defeat', 88)\n",
      "('streets', 89)\n",
      "('humour', 90)\n",
      "('manliness', 91)\n",
      "('or', 92)\n",
      "('living', 93)\n",
      "('about', 94)\n",
      "('soldiers', 95)\n",
      "('suburbs', 96)\n",
      "('.', 97)\n",
      "('born', 98)\n",
      "('to', 99)\n",
      "('deals', 100)\n",
      "('years', 101)\n",
      "('killed', 102)\n",
      "('a', 103)\n",
      "('bronze', 104)\n",
      "('grandfather', 105)\n",
      "('bearded', 106)\n",
      "('can', 107)\n",
      "('long', 108)\n",
      "('just', 109)\n",
      "('my', 110)\n",
      "('yourself', 111)\n",
      "('dreams', 112)\n",
      "('whatever', 113)\n",
      "('bitterness', 114)\n",
      "('adversities', 115)\n",
      "('has', 116)\n",
      "('ghosts', 117)\n",
      "('jagged', 118)\n",
      "(';', 119)\n",
      "(':', 120)\n",
      "('through', 121)\n",
      "('lean', 122)\n",
      "('heart', 123)\n",
      "('trying', 124)\n",
      "('wrapped', 125)\n",
      "('frontier', 126)\n",
      "('loyalty', 127)\n",
      "('traffics', 128)\n",
      "('you', 129)\n",
      "(' ', 130)\n",
      "('is', 131)\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "index = 0\n",
    "\n",
    "for token in set(pretext):\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "\n",
    "for token, index in vocab.items():\n",
    "    print((token, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc3a20",
   "metadata": {},
   "source": [
    "We see, the index is increasing. That's the **set** make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe6af6",
   "metadata": {},
   "source": [
    "And here we can simply define a raw encode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ed6e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(target_token, vocab):\n",
    "    for (token, id) in vocab.items():\n",
    "        if token == target_token:\n",
    "            return id\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09d06e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "target_token = 'hold'\n",
    "index = encode(target_token, vocab)\n",
    "print(index == vocab['hold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de7743",
   "metadata": {},
   "source": [
    "## 2 Decode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9509699",
   "metadata": {},
   "source": [
    "We still use the poetry for corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bd9c115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can I hold you with?\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bedcaf",
   "metadata": {},
   "source": [
    "Here we can simply reuse our `pretext`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481e0a10",
   "metadata": {},
   "source": [
    "This time. We do decoding. Decoding is to get the id(index) and we know its token. So in fact, it's a (index, token) dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48b2d9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '')\n",
      "(1, 'bullets')\n",
      "(2, 'am')\n",
      "(3, 'books')\n",
      "(4, 'with')\n",
      "(5, 'danger')\n",
      "(6, 'memory')\n",
      "(7, 'saved')\n",
      "(8, 'who')\n",
      "(9, 'yellow')\n",
      "(10, \"'\")\n",
      "(11, 'joy')\n",
      "(12, 'never')\n",
      "(13, 'in')\n",
      "(14, 'dead')\n",
      "(15, 'Aires')\n",
      "(16, 'sunset')\n",
      "(17, 'give')\n",
      "(18, 'on')\n",
      "(19, 'father')\n",
      "(20, 'by')\n",
      "(21, 'darkness')\n",
      "(22, 'not')\n",
      "(23, 'time')\n",
      "(24, 'uncertainty')\n",
      "(25, '\\n')\n",
      "(26, 'desperate')\n",
      "(27, 'of')\n",
      "(28, 'the')\n",
      "(29, 's')\n",
      "(30, 'untouched')\n",
      "(31, 'before')\n",
      "(32, 'heading')\n",
      "(33, 'hold')\n",
      "(34, 'have')\n",
      "(35, 'looked')\n",
      "(36, 'moon')\n",
      "(37, 'two')\n",
      "(38, 'his')\n",
      "(39, 'vanished')\n",
      "(40, 'kernel')\n",
      "(41, 'been')\n",
      "(42, 'sunsets')\n",
      "(43, 'seen')\n",
      "(44, 'hunger')\n",
      "(45, ',')\n",
      "(46, '?')\n",
      "(47, 'lungs')\n",
      "(48, 'news')\n",
      "(49, 'ancestors')\n",
      "(50, 'I')\n",
      "(51, 'words')\n",
      "(52, 'bribe')\n",
      "(53, 'life')\n",
      "(54, 'now')\n",
      "(55, 'hide')\n",
      "(56, 'insight')\n",
      "(57, 'twentyfour')\n",
      "(58, 'horses')\n",
      "(59, 'myself')\n",
      "(60, '--')\n",
      "(61, 'Buenos')\n",
      "(62, 'somehow')\n",
      "(63, 'rose')\n",
      "(64, 'authentic')\n",
      "(65, 'man')\n",
      "(66, 'central')\n",
      "(67, 'What')\n",
      "(68, 'lonely')\n",
      "(69, 'cow')\n",
      "(70, 'men')\n",
      "(71, 'and')\n",
      "(72, 'may')\n",
      "(73, 'loneliness')\n",
      "(74, 'mother')\n",
      "(75, 'at')\n",
      "(76, 'loyal')\n",
      "(77, 'explanations')\n",
      "(78, 'theories')\n",
      "(79, 'honoured')\n",
      "(80, 'hundred')\n",
      "(81, 'charge')\n",
      "(82, 'were')\n",
      "(83, 'surprising')\n",
      "(84, 'three')\n",
      "(85, 'offer')\n",
      "(86, 'Peru')\n",
      "(87, 'that')\n",
      "(88, 'defeat')\n",
      "(89, 'streets')\n",
      "(90, 'humour')\n",
      "(91, 'manliness')\n",
      "(92, 'or')\n",
      "(93, 'living')\n",
      "(94, 'about')\n",
      "(95, 'soldiers')\n",
      "(96, 'suburbs')\n",
      "(97, '.')\n",
      "(98, 'born')\n",
      "(99, 'to')\n",
      "(100, 'deals')\n",
      "(101, 'years')\n",
      "(102, 'killed')\n",
      "(103, 'a')\n",
      "(104, 'bronze')\n",
      "(105, 'grandfather')\n",
      "(106, 'bearded')\n",
      "(107, 'can')\n",
      "(108, 'long')\n",
      "(109, 'just')\n",
      "(110, 'my')\n",
      "(111, 'yourself')\n",
      "(112, 'dreams')\n",
      "(113, 'whatever')\n",
      "(114, 'bitterness')\n",
      "(115, 'adversities')\n",
      "(116, 'has')\n",
      "(117, 'ghosts')\n",
      "(118, 'jagged')\n",
      "(119, ';')\n",
      "(120, ':')\n",
      "(121, 'through')\n",
      "(122, 'lean')\n",
      "(123, 'heart')\n",
      "(124, 'trying')\n",
      "(125, 'wrapped')\n",
      "(126, 'frontier')\n",
      "(127, 'loyalty')\n",
      "(128, 'traffics')\n",
      "(129, 'you')\n",
      "(130, ' ')\n",
      "(131, 'is')\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "vocab = {}\n",
    "\n",
    "for token in set(pretext):\n",
    "    vocab[index] = token\n",
    "    index += 1\n",
    "\n",
    "for (id, token) in vocab.items():\n",
    "    print((id, token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e96982",
   "metadata": {},
   "source": [
    "Define a simple raw decode function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73259e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(target_id, vocab):\n",
    "    for (id, token) in vocab.items():\n",
    "        if id == target_id:\n",
    "            return token\n",
    "    return '<|not_found|>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee667db3",
   "metadata": {},
   "source": [
    "And here we use a special token: '<|not_found|>' to stand for the id not in the vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd83bc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "id = 66\n",
    "token = decode(id, vocab)\n",
    "\n",
    "print(token == vocab[id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301aafb",
   "metadata": {},
   "source": [
    "## 3 Implement a tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f84f17c",
   "metadata": {},
   "source": [
    "Till now, we know how the `encode` do, and `decode` do. So we can implement a tokenzier now. For different corpus we have different tokenzier, so the tokenizer must have a parameter to note the corpus. The `encode` and `decode` function are demanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd8c90f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tmpTokenizer:\n",
    "    # Note here: the corpus not the raw text, it should be preprocessed.\n",
    "    def __init__(self, corpus):\n",
    "        self.token_id_dict = {token: id for id, token in enumerate(set(corpus))}\n",
    "        self.id_token_dict= {id: token for id, token in enumerate(set(corpus))}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # preprogress the text\n",
    "        preprocess = re.split('([.,\\'\\\"?!:;()]|--|\\s)', text)\n",
    "        \n",
    "        # remove the token with blank like this: __hello____world__.\n",
    "        # here use '_' to replace blank\n",
    "        preprocess = [\n",
    "            item.strip() for item in preprocess if item.strip()\n",
    "        ]\n",
    "\n",
    "        # generate ids\n",
    "        ids = [self.token_id_dict[token] for token in preprocess]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # this function will join the blank string` to the list iterablly.\n",
    "        # that will automatically to let token final sentence to have blank\n",
    "        # nor it generation will like `whatcanIholdyouwith`\n",
    "        text = \" \".join([self.id_token_dict[id] for id in ids])\n",
    "\n",
    "        # remove blank in front of punctuation mark\n",
    "        text = re.sub(r'\\s+([.,?!;:\\'\\\"])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba0ac058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What can I hold you with?\n"
     ]
    }
   ],
   "source": [
    "text = 'What can I hold you with?'\n",
    "tokenizer = tmpTokenizer(pretext)\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5046ae45",
   "metadata": {},
   "source": [
    "## 3 Add special token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e1ad8f",
   "metadata": {},
   "source": [
    "In the very begin, we implemnt a encode function, that if a token is not in our corpus, that will return $-1$, and if we append $-1$ to our corpus, and it will corresponding to one sepical token. What is it? So here, we introduce the **sepcial token**\n",
    "<br>\n",
    "Generally, we use those tokens:\n",
    "- **'<|unk|>'**: represent unknow token\n",
    "\n",
    "And we will also use one special token to represent the end of sentence. We called **'<|endoftext|>'**\n",
    "And if the model has the thinking function. And we will use token: **'<think>'** to stand for the thinking begin. And use **'</think>'** like this:\n",
    "<br>\n",
    "![](https://cdn.jsdelivr.net/gh/gzqccnu/img/image-20250730203818149.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b49721",
   "metadata": {},
   "source": [
    "And now we can expand the `pretext` with new token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6a84116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before length: 531\n",
      "now length: 533\n"
     ]
    }
   ],
   "source": [
    "before = len(pretext)\n",
    "pretext.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "now = len(pretext)\n",
    "print(\"before length:\", before)\n",
    "print(\"now length:\", now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffea140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dest = \"../corpus/pretext.txt\"\n",
    "directory = os.path.dirname(dest)\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "with open(dest, 'w', encoding=\"utf-8\") as file:\n",
    "    file.write(str(sorted(set(pretext))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f376228",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {token: id for id, token in enumerate(pretext)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5501449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532\n",
      "531\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"<|unk|>\"])\n",
    "print(vocab[\"<|endoftext|>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c17e02",
   "metadata": {},
   "source": [
    "So, we can fix the tmpTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33e25dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    # Note here: the corpus not the raw text, it should be preprocessed.\n",
    "    def __init__(self, corpus):\n",
    "        self.token_id_dict = {token: id for id, token in enumerate(sorted(set(corpus)))}\n",
    "        self.id_token_dict= {id: token for id, token in enumerate(sorted(set(corpus)))}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        # preprogress the text\n",
    "        preprocess = re.split('([.,\\'\\\"?!:;()]|--|\\s)', text)\n",
    "        \n",
    "        # remove the token with blank like this: __hello____world__.\n",
    "        # here use '_' to replace blank\n",
    "        preprocess = [\n",
    "            item.strip() for item in preprocess if item.strip()\n",
    "        ]\n",
    "\n",
    "        # Add \"<|unk|>\" token\n",
    "        preprocess = [\n",
    "            item if item in self.token_id_dict\n",
    "            else \"<|unk|>\" for item in preprocess\n",
    "        ]\n",
    "\n",
    "        # generate ids\n",
    "        ids = [self.token_id_dict[token] for token in preprocess]\n",
    "\n",
    "        return ids\n",
    "    \n",
    "    def decode(self, ids):\n",
    "        # this function will join the blank string` to the list iterablly.\n",
    "        # that will automatically to let token final sentence to have blank\n",
    "        # nor it generation will like `whatcanIholdyouwith`\n",
    "        text = \" \".join([self.id_token_dict[id] for id in ids])\n",
    "\n",
    "        # remove blank in front of punctuation mark\n",
    "        text = re.sub(r'\\s+([.,?!;:\\'\\\"])', r'\\1', text)\n",
    "\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "806d9cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I offer you <|unk|> and <|unk|>.\n"
     ]
    }
   ],
   "source": [
    "text = \"I offer you tokenizer and LLM.\"\n",
    "\n",
    "tokenizer = Tokenizer(pretext) \n",
    "print(tokenizer.decode(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0eb9b",
   "metadata": {},
   "source": [
    "## 4 Using pretrained tokenzier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "44eee39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3937cf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41fc653b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151646, 40, 3010, 498, 16164, 323, 16895, 11, 646, 498, 7942, 879, 1079, 358, 30, 21607, 0, 358, 2776, 45958, 0]\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./deepseek\"\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "\n",
    "text = \"I offer you encode and decode, can you guess who am I? Yeah! I'm tokenizer!\"\n",
    "\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5eef21c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<｜begin▁of▁sentence｜>I offer you encode and decode, can you guess who am I? Yeah! I'm tokenizer!\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(\"I offer you encode and decode, can you guess who am I? Yeah! I'm tokenizer!\")))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
