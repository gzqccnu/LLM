# infer

This subsection more focus on locally run a LLM. Not for industrial-level deployment.